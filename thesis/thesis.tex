\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color,soul}
\sethlcolor{cyan}
%\usepackage{natbib}
\bibliographystyle{apalike}
%\setcitestyle{authoryear,open={((},close={))}}

\newcommand{\delims}[3]{\!\left #1 #2 \right #3}
\newcommand{\parens}[1]{\delims{(}{#1}{)}}
\newcommand{\braces}[1]{\delims{\lbrace}{#1}{\rbrace}}
\newcommand{\brackets}[1]{\delims{[}{#1}{]}}
\newcommand{\norm}[1]{\delims{\|}{#1}{\|}}
\newcommand{\abs}[1]{\delims{|}{#1}{|}}
\newcommand{\inner}[2]{\delims{\langle}{#1,#2}{\rangle}}
\newcommand{\vectinner}[2]{\inner{\vect{#1}}{\vect{#2}}}

\newcommand{\vect}[1]{\mathbf{#1}}

\newcommand{\nquote}[1]{``{#1}''}

\begin{document}
\title{\vspace{-10ex}Noncausal PixelCNN}
\author{Dmitry Brizhinev}
\maketitle

\section{Abstract}
I introduce a modification to the generative model PixelCNN, which allows generation to complete in constant time, rather than requiring a separate forward pass for each pixel of the output. The key idea is to not enforce a strict causal order like in PixelCNN, effectively allowing generated pixels to depend on both the past and the future. I show empirically that, despite violating causality, the resulting model functions \hl{comparably} to PixelCNN. This improvement markedly improves the running time of PixelCNN in its core task -- generation -- and can also perform tasks that PixelCNN cannot, such as denoising, deblurring, and upsampling.

\section{Introduction}

The generation of synthetic image (and other) data, is an important step on the way to complete machine understanding of images (and others) \cite{??}. In the deep learning field, there are currently three broad approaches to the generation of images: variational autoencoders (VAEs) \cite{??}, generative adversarial networks (GANs) \cite{??} and autoregressive models like PixelCNN \cite{??}.

\subsection{The Natural Image Manifold}

All three approaches attempt to solve the problem of how a neural network can both \emph{represent} the set of \nquote{natural} images, and to \emph{sample} from it. It is common to think of natural images as defining a manifold (continuous subset) in the space of all possible images. Importantly, this manifold is not convex -- the average of two natural images is often not a natural image itself. A network trained to generate natural images implicitly defines the shape of a manifold, which we hope is as close as possible to the \nquote{true} natural image manifold. Thinking in terms of the natural image manifold is fruitful -- for example, \cite{??} apply this idea to image editing, using a GAN to constrain edits to remain within the manifold -- that is, unlike ordinary per-pixel edits which quickly make the image look doctored and unrealistic, their system tries to approximate what the user wants while ensuring that the image remains natural at all times.

\hl{diagram}

\subsection{The Problem of Sampling}

Consider the simplest possible approach to generating images. We train a deterministic, feedforward network with a fixed input to produce the desired images as output (say, with L2 loss). The problem here is clear: the network can only ever produce one output, and so the optimal strategy (to minimise L2 loss) is to output the mean of all the images in the training set. This network fails to learn much about natural images.

\hl{diagrams throughout this section, my idea of a diagram for two related pixels}

Now suppose the deterministic network can receive a random seed as input. For each training example, we pair one of our images with a random input seed. But now the network still has no choice but to output the mean: the seeds provide it with no information about the images it needs to generate. If we instead associate each image with a unique seed, and make sure the same seed appears each time the same image is seen, the network now has a chance, but we deprive it of the ability to learn any structure in the image space -- it has to memorise the mapping from seeds to images, and will not generalise.

Finally, suppose we let the network be probabilistic. Instead of outputting images, the network outputs probability distributions over each pixel (with cross-entropy loss). We still face a problem here: the network no longer has to output the mean, but the best it can do is to learn the distribution of each pixel independently. The network cannot learn even as simple a relationship as that two particular neighbouring pixels are always the same colour. If each pixel can be white or black with 50\% probability, the best it can do is predict a 50\% probability of each colour for each pixel. Simply making the network probabilistic still fails to capture dependencies \emph{among} pixels.

\hl{Explain more the problem of generating when there are multiple plausible outputs for one input; and explain for each of the three below how they deal with this}

The three main approaches to generation all solve this problem in different ways.

\subsection{Variational Autoencoder}

The variational autoencoder \cite{??} is structured as follows: \hl{diagram}. An \nquote{encoder} network turns images into low-dimensional seeds (called \nquote{latent vectors}). A \nquote{decoder} network learns to turn the seeds back into images. Both networks are trained simultaneously end-to-end. The latent vector must be low-dimensional to prevent the network from simply recording the image itself into the vector. This is an ordinary autoencoder, and a variational autoencoder adds the constraint that the latent vectors must overall have some known distribution.

How does this structure solve the problem above? Now, instead of choosing random seeds, we let the network associate seeds (latent vectors) to images. This allows the network to assign similar latent vectors to similar images, and thus take advantage of the structure of the image space. With this architecture, we can expect that latent vectors outside the training set will still produce natural images. The constraint that the latent vectors have a known distribution allows us to sample from the natural image manifold, as we wanted to.

VAEs still suffer from producing blurry samples. The reason can be illustrated with a diagram. \hl{diagram}. Suppose a particular latent vector corresponds to two plausible natural images. The network's best (loss-minimising) option is to output the mean of these two images. There is no force ensuring that every individual sample be a natural image, only that loss is minimised overall -- as long as the mapping of latent vectors to natural images is not perfect, blurring results.

\subsection{Generative Adversarial Networks}

GANs have produced the most spectacular generation results, causing a surge of interest in the idea. A GAN consists of two networks: a \nquote{generator} that takes random seeds (latent vectors) as input and produces natural images as output, and a \nquote{discriminator} that takes natural images \emph{or} generator samples as input and tries to tell which is which. Crucially, the generator is never trained on \emph{particular} images. It is free to learn whatever mapping it wants from latent vectors to images, as long as the outputs fool the discriminator. In essence, instead of training the generator on particular pairs of images, we train a differentiable metric -- the discriminator -- that compares each of the generator's samples to the entire training set. \hl{expand on this and how true KL divergence or similar is expensive but could solve e.g. VAE blurriness}. The networks are trained end-to-end. To sample images, we simply sample a latent vector from the same distribution we used at training time.

How does this structure solve the problem? Now the generator has to generate images with global consistency, otherwise the discriminator can beat it. We don't suffer from bluriness because we never ask the generator to generate a \emph{particular} image. Thus, the network never has to compromise and take the mean of the images we want. The flip side is the well-known difficulty with GANs: \emph{mode collapse}. The basic setup described above provides no incentive for the generator to include the whole image manifold in its range of outputs. It is merely judged on whether the particular images it \emph{does} produce look natural. It is not penalised for deciding to \emph{never} produce a certain image. This problem has spawned a lot of research into encouraging \nquote{diversity} in GAN samples, e.g. with additional terms in the loss function.

\subsection{Autoregressive Models}

\hl{- Review of generation methods: autoregressive (pixelCNN)
  
  - Exact and approximate log likelihood
  
- Review of PixelCNN and applications/extensions

  - PixelCNN++, WaveNet, Video, Translation, Quantiles}

\section{Motivation}

The PixelCNN model has three major flaws that I attempt to address. First, generation takes time linear in the number of pixels, since each pixel must be generated to be fed as input to the next pixel. Second, it has an aesthetically displeasing asymmetry. By default, PixelCNN generates pixels from top-left to bottom-right. As a result, PixelCNN can convincingly fill-in the bottom half of an image given the top half (the examples usually displayed in papers) but not vice-versa. Third, the model cannot \emph{correct} pixels that are only slightly wrong, and cannot infer which pixels in an image are wrong -- it can \nquote{fill-in} missing parts of an image only if the location of the missing region is known.

The first and second flaws (slow generation, asymmetry) are closely related. They both stem from defining an order on the pixels, and requiring each pixel to depend (only) on the preceding pixels. This is a nice idea, since it allows for an \emph{exact} factorisation of the conditional probability of an image \hl{equation}. But it comes with the aforementioned costs. Although the bottom-left pixel can depend on the values on all the others, the top-left pixel can only ever be generated independently, even if the rest of the image is available. And, of course, pixels have to be generated one at a time.

In theory, we could fill in the early pixels by performing Bayesian inference -- given the later pixels in the image, what are the most likely early pixels that could have produced them? However, such inference is computationally intractable. In fact, we use PixelCNN precisely to approximate the ideal Bayesian computation of the conditional likelihoods of all the pixels in the image. Unfortunately, the model is not easily invertible.

What is the alternative? What would it mean if pixels did not have a fixed order? In that case, a generated pixel could depend on the values of \nquote{future} pixels that have not been generated yet i.e. the model is no longer \nquote{causal}. This seems like a contradiction, but what if we fed some other value as a placeholder for missing pixels? In the simplest possible case, suppose we feed noise. Then, after the first forward pass, each pixel is generated using only noise as input. But we can do another pass, and now each pixel receives the first generated values as input. On the second pass, we hope that pixels will now be generated to conform to the choices made in the first pass. This procedure can be iterated a few (constant) number of times, and we can hope that after a few passes the generated image will converge to something reasonable. This procedure is not theoretically \hl{analysis} guaranteed to converge in constant time, or at all, but in this work I show empirically that it does.

The approach of generating all the pixels in parallel and replacing missing ones with noise has another surprising benefit. It also addresses the third issue (the need to explicitly mark missing pixels). The resulting model simply takes one image and outputs another. The input image can be noise, if we want to generate something from scratch. But it doesn't have to be. We can also feed a noisy, blurry, or downsampled version of an image, and try to generate a clean one.

\hl{talk about causality in autoregression}


\hl{- Motivation for my model as a modification of PixelCNN:

  - view of my model as several parallel iterations of PixelCNN
  
  - Approximate version of the ideal where we train on one pixel at a time
  
  - can do other operations including denoising that PixelCNN can't do because it needs to know which pixels are accurate.
  
  - Training is harder because we don't want to only learn identity}
  
\section{Alternative Views}

\hl{- Other ways to look at my model:

  - Denoising autoencoder without a bottleneck
  
  - Combination of VAE and PixelCNN due to the noise inputs
  
  - Projection operator into natural image space + theoretical discussion of convergence
  
  - Measuring loss ONLY on noise pixels is a bad idea because we want identity on true natural images
  
  - Weird kind of recurrent CNN
  
  - Model of neural field with complex excitatory/inhibitory connections}

\section{Literature Review}

\subsection{Generative models}

\hl{- Review of generation methods: autoregressive (pixelCNN), GAN, VAE
  
  - View of generation as defining a "natural image" manifold
  
  - Exact and approximate log likelihood
  
- Review of PixelCNN and applications/extensions

  - PixelCNN++, WaveNet, Video, Translation, Quantiles}

\subsection{Autoregressive models}

\subsection{Noncausal autoregression}

\hl{- Review of noncausal autoregression:

  - economics and the old autoregressive models
  
  - (un)importance of causality, for stock market data (most causal), audio/text/video (less causal), images (least causal).
  
  - bidirectional RNN}

\subsection{Neural fields}

\hl{- Review of neural fields in neuroscience and in AI}

\subsection{Recurrent Convolutional networks}

\hl{- Review of recurrent convolutional networks}

\subsection{Denoising, deblurring, and upsampling}

\hl{- Review of denoising/deblurring/upsampling}

\subsection{Dropout and uncertainty}

\hl{- ????? Review of dropout uncertainty ????}

\section{Methods}

\hl{- Architecture details, hyperparameters}

\section{Results}

\hl{- Results

  - Hopefully even if I don't beat PixelCNN, I will demonstrate that it's not too much worse, but runs in constant time and can do a bunch of things pixelcnn can't
  
  - Include training curves}
  
\section{Future Work}

\hl{- Possible improvements and future work

  - PixelCNN++: dropout, logistic mixture output, bottleneck
  
  - Recurrence within layers (and better kind of neural field)
  
  - Batch normalisation
  
  - ??????? Dropout / batch norm uncertainty ???????
  
  - Deterministic model relying entirely on latent noise info, or training whole model end-to-end (I think this is a bad idea and will cause more bluriness)
  
  - Other datasets: imagenet/cifar, audio (speech/music), text, video
  
  - Elaborating based on sketches
  
  - Editing in the image manifold
  
  - Sparsity regularisation for the output layer getting stronger as we go deeper
  
  - Reducing temperature as we go deeper
  
  - Removing random noise in inputs with an extra mask telling us which inputs to ignore
  
  - Other regularisations - weight decay/L2 ?
  
  - gradient clipping
  
  - Better losses - wesserstein distance}

\bibliography{../references}
\end{document}