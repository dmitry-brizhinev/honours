\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color,soul}
\sethlcolor{cyan}
%\usepackage{natbib}
\bibliographystyle{apalike}
%\setcitestyle{authoryear,open={((},close={))}}

\newcommand{\delims}[3]{\!\left #1 #2 \right #3}
\newcommand{\parens}[1]{\delims{(}{#1}{)}}
\newcommand{\braces}[1]{\delims{\lbrace}{#1}{\rbrace}}
\newcommand{\brackets}[1]{\delims{[}{#1}{]}}
\newcommand{\norm}[1]{\delims{\|}{#1}{\|}}
\newcommand{\abs}[1]{\delims{|}{#1}{|}}
\newcommand{\inner}[2]{\delims{\langle}{#1,#2}{\rangle}}
\newcommand{\vectinner}[2]{\inner{\vect{#1}}{\vect{#2}}}

\newcommand{\vect}[1]{\mathbf{#1}}

\newcommand{\nquote}[1]{``{#1}''}

\begin{document}
\title{\vspace{-10ex}Noncausal PixelCNN}
\author{Dmitry Brizhinev}
\maketitle

\section{Abstract}
I introduce a modification to the generative model PixelCNN, which allows generation to complete in constant time, rather than requiring a separate forward pass for each pixel of the output. The key idea is to not enforce a strict causal order like in PixelCNN, effectively allowing generated pixels to depend on both the past and the future. I show empirically that, despite violating causality, the resulting model functions \hl{comparably} to PixelCNN. This improvement markedly improves the running time of PixelCNN in its core task -- generation -- and can also perform tasks that PixelCNN cannot, such as denoising, deblurring, and upsampling.

\section{Introduction}

\cite{??}

\subsection{PixelCNN}

\hl{- Review of generation methods: autoregressive (pixelCNN), GAN, VAE

  - Explain how they approach the problem of multiple outputs for one input
  
  - Diagram for two related pixels
  
  - View of generation as defining a "natural image" manifold
  
  - Exact and approximate log likelihood
  
- Review of PixelCNN and applications/extensions

  - PixelCNN++, WaveNet, Video, Translation, Quantiles}

\section{Motivation}

The PixelCNN model has three major flaws that I attempt to address. First, generation takes time linear in the number of pixels, since each pixel must be generated to be fed as input to the next pixel. Second, it has an aesthetically displeasing asymmetry. By default, PixelCNN generates pixels from top-left to bottom-right. As a result, PixelCNN can convincingly fill-in the bottom half of an image given the top half (the examples usually displayed in papers) but not vice-versa. Third, the model cannot \emph{correct} pixels that are only slightly wrong, and cannot infer which pixels in an image are wrong -- it can \nquote{fill-in} missing parts of an image only if the location of the missing region is known.

The first and second flaws (slow generation, asymmetry) are closely related. They both stem from defining an order on the pixels, and requiring each pixel to depend (only) on the preceding pixels. This is a nice idea, since it allows for an \emph{exact} factorisation of the conditional probability of an image \hl{equation}. But it comes with the aforementioned costs. Although the bottom-left pixel can depend on the values on all the others, the top-left pixel can only ever be generated independently, even if the rest of the image is available. And, of course, pixels have to be generated one at a time.

In theory, we could fill in the early pixels by performing Bayesian inference -- given the later pixels in the image, what are the most likely early pixels that could have produced them? However, such inference is computationally intractable. In fact, we use PixelCNN precisely to approximate the ideal Bayesian computation of the conditional likelihoods of all the pixels in the image. Unfortunately, the model is not easily invertible.

What is the alternative? What would it mean if pixels did not have a fixed order? In that case, a generated pixel could depend on the values of \nquote{future} pixels that have not been generated yet i.e. the model is no longer \nquote{causal}. This seems like a contradiction, but what if we fed some other value as a placeholder for missing pixels? In the simplest possible case, suppose we feed noise. Then, after the first forward pass, each pixel is generated using only noise as input. But we can do another pass, and now each pixel receives the first generated values as input. On the second pass, we hope that pixels will now be generated to conform to the choices made in the first pass. This procedure can be iterated a few (constant) number of times, and we can hope that after a few passes the generated image will converge to something reasonable. This procedure is not theoretically \hl{analysis} guaranteed to converge in constant time, or at all, but in this work I show empirically that it does.

The approach of generating all the pixels in parallel and replacing missing ones with noise has another surprising benefit. It also addresses the third issue (the need to explicitly mark missing pixels). The resulting model simply takes one image and outputs another. The input image can be noise, if we want to generate something from scratch. But it doesn't have to be. We can also feed a noisy, blurry, or downsampled version of an image, and try to generate a clean one.


\hl{- Motivation for my model as a modification of PixelCNN:

  - view of my model as several parallel iterations of PixelCNN
  
  - Approximate version of the ideal where we train on one pixel at a time
  
  - can do other operations including denoising that PixelCNN can't do because it needs to know which pixels are accurate.
  
  - Training is harder because we don't want to only learn identity}
  
\section{Alternative Views}

\hl{- Other ways to look at my model:

  - Denoising autoencoder without a bottleneck
  
  - Combination of VAE and PixelCNN due to the noise inputs
  
  - Projection operator into natural image space + theoretical discussion of convergence
  
  - Measuring loss ONLY on noise pixels is a bad idea because we want identity on true natural images
  
  - Weird kind of recurrent CNN
  
  - Model of neural field with complex excitatory/inhibitory connections}

\section{Literature Review}

\subsection{Generative models}

\hl{- Review of generation methods: autoregressive (pixelCNN), GAN, VAE
  - Explain how they approach the problem of multiple outputs for one input
  - Diagram for two related pixels
  - View of generation as defining a "natural image" manifold
  - Exact and approximate log likelihood
- Review of PixelCNN and applications/extensions
  - PixelCNN++, WaveNet, Video, Translation, Quantiles

\subsection{Autoregressive models}

\subsection{Noncausal autoregression}

- Review of noncausal autoregression:
  - economics and the old autoregressive models
  - (un)importance of causality, for stock market data (most causal), audio/text/video (less causal), images (least causal).
  - bidirectional RNN

\subsection{Neural fields}

- Review of neural fields in neuroscience and in AI

\subsection{Recurrent Convolutional networks}

- Review of recurrent convolutional networks

\subsection{Denoising, deblurring, and upsampling}

- Review of denoising/deblurring/upsampling

\subsection{Dropout and uncertainty}

- ????? Review of dropout uncertainty ????

\section{Methods}

- Architecture details, hyperparameters

\section{Results}

- Results
  - Hopefully even if I don't beat PixelCNN, I will demonstrate that it's not too much worse, but runs in constant time and can do a bunch of things pixelcnn can't
  - Include training curves
  
\section{Future Work}
- Possible improvements and future work
  - PixelCNN++: dropout, logistic mixture output, bottleneck
  - Recurrence within layers (and better kind of neural field)
  - Batch normalisation
  - ??????? Dropout / batch norm uncertainty ???????
  - Deterministic model relying entirely on latent noise info, or training whole model end-to-end (I think this is a bad idea and will cause more bluriness)
  - Other datasets: imagenet/cifar, audio (speech/music), text, video
  - Elaborating based on sketches
  - Editing in the image manifold
  - Sparsity regularisation for the output layer getting stronger as we go deeper
  - Reducing temperature as we go deeper
  - Removing random noise in inputs with an extra mask telling us which inputs to ignore
  - Other regularisations - weight decay/L2 ?
  - gradient clipping
  - Better losses - wesserstein distance

\bibliography{../references}
\end{document}