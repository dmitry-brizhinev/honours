\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{color,soul}
\sethlcolor{cyan}
%\usepackage{natbib}
\bibliographystyle{apalike}
%\setcitestyle{authoryear,open={((},close={))}}

\newcommand{\delims}[3]{\!\left #1 #2 \right #3}
\newcommand{\parens}[1]{\delims{(}{#1}{)}}
\newcommand{\braces}[1]{\delims{\lbrace}{#1}{\rbrace}}
\newcommand{\brackets}[1]{\delims{[}{#1}{]}}
\newcommand{\norm}[1]{\delims{\|}{#1}{\|}}
\newcommand{\abs}[1]{\delims{|}{#1}{|}}
\newcommand{\inner}[2]{\delims{\langle}{#1,#2}{\rangle}}
\newcommand{\vectinner}[2]{\inner{\vect{#1}}{\vect{#2}}}

\newcommand{\vect}[1]{\mathbf{#1}}

\newcommand{\nquote}[1]{``{#1}''}

\begin{document}
\title{\vspace{-10ex}Noncausal PixelCNN}
\author{Dmitry Brizhinev}
\maketitle

\section*{Abstract}
I introduce a modification to the generative model PixelCNN, which allows generation to complete in constant time, rather than requiring a separate forward pass for each pixel of the output. The key idea is to not enforce a strict causal order like in PixelCNN, effectively allowing generated pixels to depend on both the past and the future. I show empirically that, despite violating causality, the resulting model functions \hl{comparably} to PixelCNN. This improvement markedly improves the running time of PixelCNN in its core task -- generation -- and can also perform tasks that PixelCNN cannot, such as denoising and filling-in the top half of an image.

\tableofcontents

\section{Introduction}

The generation of synthetic image (and other) data, is an important step on the way to complete machine understanding of images (and other media). In the deep learning field, there are currently three broad popular approaches to the generation of images: variational autoencoders (VAEs) \cite{vae}, generative adversarial networks (GANs) \cite{gan} and autoregressive models like PixelCNN \cite{pixelcnn2}.

\subsection{The Natural Image Manifold}

All three approaches attempt to solve the problem of how a neural network can both \emph{represent} the set of \nquote{natural} images, and to \emph{sample} from it. It is common to think of natural images as defining a manifold (continuous subset) in the space of all possible images \cite{manifoldmanipulation,imageinpainting,manifoldmixup}. Importantly, this manifold is not convex -- the average of two natural images is often not a natural image itself (see Figure \ref{manifold}). A network trained to generate natural images implicitly defines the shape of a manifold \cite{manifoldmanipulation}, which we hope is as close as possible to the \nquote{true} natural image manifold. Thinking in terms of the natural image manifold is fruitful -- for example, \cite{manifoldmanipulation} apply this idea to image editing, using a GAN to constrain edits to remain within the manifold -- that is, unlike ordinary per-pixel edits which quickly make the image look doctored and unrealistic, their system tries to approximate what the user wants while ensuring that the image remains natural at all times.

\begin{figure}
	\centering
	\vspace{-3cm}	
	
    \begin{subfigure}{0.7\columnwidth}
        \centering
        \caption{Pixel space}
        \includegraphics[width=\linewidth]{../../Diagrams/manifold_pixel.png} 
        \label{manifold:pixel}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    
    \begin{subfigure}{0.7\columnwidth}
        \centering
        \caption{Perceptual space}
        \includegraphics[width=\linewidth]{../../Diagrams/manifold_perceptual.png} 
        \label{manifold:perceptual}
    \end{subfigure}
  \caption{A simplified schematic of the natural image manifold. (\subref{manifold:pixel}) The manifold in pixel space (i.e. each dimension is the value of a pixel). In reality this would be a $3N$ dimensional space for images with $N$ pixels of 3 channels each. Crucially, the manifold is not convex -- the average of a cat and dog image in pixel space is not a natural image. (\subref{manifold:perceptual}) The same manifold in a different space, something where the dimensions are perceptually meaningful. In this case, if the dimensions are appropriate, the average of a cat and a dog image might be an image of something that looks both cat- and dog-like. We generally hope that the dimensions of the latent vector of a VAE or a GAN will form such a perceptual space.}
  \label{manifold}
\end{figure}


\subsection{The Problem of Sampling}

\begin{figure}
  \centering
  \vspace{-3cm}
  
      \begin{subfigure}{0.7\columnwidth}
        \centering
        \caption{Traffic light dataset}
        \includegraphics[width=\linewidth]{../../Diagrams/lights_intro.png} 
        \label{trafficlights:example}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Data space}
        \includegraphics[width=\linewidth]{../../Diagrams/lights_example.png} 
        \label{trafficlights:axes}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Traffic light manifold}
        \includegraphics[width=\linewidth]{../../Diagrams/lights_plain.png} 
        \label{trafficlights:manifold}
    \end{subfigure}
  \caption{I will use this example for the rest of this section. (\subref{trafficlights:example}) Imagine that our dataset is these traffic lights. When we generate traffic lights, they must either have a green light or a red light. They should not have neither, both, or an average of the two. (\subref{trafficlights:axes}) With only two dimensions of variation -- the brightness of the green and red lights -- we can represent the complete space of possible data points. (\subref{trafficlights:manifold}) The valid regions are the top-left (only green) and bottom-right (only red), so the \nquote{traffic light manifold} is those two areas.}
  \label{trafficlights}
\end{figure}

Consider the simplest possible approach to generating images. We train a deterministic, feedforward network with a fixed input to produce the desired images (Figure \ref{trafficlights}) as output (say, with L2 loss). The problem here is clear: the network can only ever produce one output, and so the optimal strategy (to minimise L2 loss) is to output the mean of all the images in the training set (Figure \ref{lightsmean}). This network fails to learn much about natural images.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_mean.png}
  \caption{The output of a deterministic neural network. It can only choose one point. If the network tries to optimise L2 loss, the best place is the average of the valid points. This is not a good generator. In practice, the result is blurry images.}
  \label{lightsmean}
\end{figure}

Now suppose the deterministic network can receive a random seed as input. For each training example, we pair one of our images with a random input seed. But now the network still has no choice but to output the mean: the seeds provide it with no information about the images it needs to generate. If we instead associate each image with a unique seed, and make sure the same seed appears each time the same image is seen, the network now has a chance, but we deprive it of the ability to learn any structure in the image space -- it has to memorise the mapping from seeds to images, and will not generalise (Figure \ref{randompoints}).

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_randompoints.png}
  \caption{The output of a deterministic network that gets random values as input, where each value is paired to a fixed output. It could learn to output the correct data by memorising it, but it can't learn any of the structure, since the mapping from input to output is random. If we feed the network a new input value it has not seen before, there is no reason to expect a sane output.}
  \label{randompoints}
\end{figure}

Finally, suppose we let the network be probabilistic. Instead of outputting images, the network outputs probability distributions over each pixel (with cross-entropy loss). We still face a problem here: the network no longer has to output the mean, but the best it can do is to learn the distribution of each pixel independently (Figure \ref{independent}). The network cannot learn even as simple a relationship as that two particular neighbouring pixels are always the same colour. If each part of the traffic light can be on or off with 50\% probability, the best it can do is predict a 50\% probability of each possibility for each pixel. Simply making the network probabilistic still fails to capture dependencies \emph{among} pixels.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_independent.png}
  \caption{The output of a network that can produce probability distributions over pixels. Because it has no way to capture dependencies among pixels, the best it can do is to capture the marginal distribution of each pixel. It can specify that the green light can be on or off, and the red light can be on or off, but it cannot specify that \emph{exactly one} of the two must be on.}
  \label{independent}
\end{figure}

The fundamental problem is that to generate images the network would have to output the full joint probability distribution of all the pixels (which is definitely not as simple as a multivariate Gaussian). This is intractable, so we have to find ways to approximate or simplify it. The three main approaches to generation all solve this problem in different ways.

\subsection{Variational Autoencoder}

The variational autoencoder \cite{vae} is structured as follows: An \nquote{encoder} network turns images into low-dimensional seeds (called \nquote{latent vectors}). A \nquote{decoder} network learns to turn the seeds back into images. Both networks are trained simultaneously end-to-end. The latent vector must be low-dimensional to prevent the network from simply recording the image itself into the vector. This is an ordinary autoencoder, and a variational autoencoder adds the constraint that the latent vectors must overall have some known distribution.

How does this structure solve the problem of sampling a joint distribution? Now, instead of choosing random seeds like in Figure \ref{randompoints}, we let the network associate seeds (latent vectors) to images. This allows the network to assign similar latent vectors to similar images, and thus take advantage of the structure of the image space. With this architecture, we can expect that latent vectors outside the training set will still produce natural images. The constraint that the latent vectors have a known distribution allows us to sample from the natural image manifold, as we wanted to.

VAEs still suffer from producing blurry samples \cite{vaeunderstanding}. The reason can be illustrated with a diagram (Figure \ref{vaemean}). Suppose a particular latent vector corresponds to two plausible natural images. The network's best (loss-minimising) option is to output the mean of these two images. There is no force ensuring that every individual sample be a natural image, only that loss is minimised overall -- as long as the mapping of latent vectors to natural images is not perfect, blurring results.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_vaemean.png}
  \caption{The output of a VAE that does not have enough space in the latent vector to encode all the images. If multiple possible data points (say, green and red lights) map to the same latent vector, the network can do no better than to \nquote{reconstruct} this latent vector into the average of the data points that map to it.}
  \label{vaemean}
\end{figure}

\subsection{Generative Adversarial Networks}

GANs \cite{gan} have produced the most spectacular generation results, causing a surge of interest in the idea \cite{imagefromcaption,unsupervisedgan,structurestylegan,texttoimagegan,imagetoimagegan,progressivegrowing}. A GAN consists of two networks: a \nquote{generator} that takes random seeds (latent vectors) as input and produces natural images as output, and a \nquote{discriminator} that takes natural images \emph{or} generator samples as input and tries to tell which is which. Crucially, the generator is never trained on \emph{particular} images. It is free to learn whatever mapping it wants from latent vectors to images, as long as the outputs fool the discriminator. In essence, instead of training the generator on particular pairs of images, we train a differentiable metric -- the discriminator -- that implicitly compares each of the generator's samples to the entire training set \cite{wgan}. The networks are trained end-to-end. To sample images, we simply sample a latent vector from the same distribution we used at training time.

How does this structure solve the problem of sampling from a joint distribution? Now the generator has to generate images with global consistency, otherwise the discriminator can beat it. We don't suffer from blurriness because we never ask the generator to generate a \emph{particular} image. Thus, the network never has to compromise and take the mean of the images we want. The flip side is the well-known difficulty with GANs: \emph{mode collapse} \cite{gantechniques}. The basic setup described above provides no incentive for the generator to include the whole image manifold in its range of outputs (Figure \ref{modecollapse}). It is merely judged on whether the particular images it \emph{does} produce look natural. It is not penalised for deciding to \emph{never} produce a certain image.\footnote{Some authors (e.g. \cite{ganmetrics}) distinguish between \emph{mode collapse} -- when multiple distinct latent vectors come to produce the same output -- and \emph{mode dropping} -- when certain regions of the output space are not represented. These two problems are closely related, and I will use \nquote{mode collapse} to refer to both.} This problem has spawned a lot of research into encouraging \nquote{diversity} in GAN samples, e.g. with additional terms in the loss function \cite{gantechniques,energygan,unrolledgan,wgan}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_modecollapse.png}
  \caption{The output of a mode-collapsed GAN. The generator is judged merely on whether its outputs are realistic. It has no incentive to make sure that it can generate all possible outputs, so it can cheat by focusing on just a subset of the output space (but modelling that region very well).}
  \label{modecollapse}
\end{figure}

\subsection{Autoregressive Models}

The autoregressive model originates from trying to solve the problems of VAEs and GANs: blurriness and mode collapse. The headline autoregressive model is PixelCNN \cite{pixelcnn2}, and variations on the same \cite{pixelcnn3,pixelcnn++,superres,wavenet,bytenet,videopixel}. An autoregressive model generates an image one pixel at a time, with each operation taking previously generated pixels as input. This requires choosing and enforcing a strict ordering on pixels, such that no pixel can ever receive information from pixels \nquote{in the future}. At each step, the network outputs a probability distribution over the next pixel. We sample from this distribution, and the actually selected pixel value becomes part of the input for the next step. Generating an image with $N$ distinct values (including multiple channels) thus takes $N$ forward passes, which must be performed serially. However, training can be parallelised, since at training time all of the true pixel values are already available. Many variations are possible on this basic idea, like using recurrent neural networks to transmit information from distant pixels \cite{pixelcnn1}, but the standard model is a many-layered convolutional network where the convolutions are carefully \emph{masked} \cite{pixelcnn2} or \emph{shifted} \cite{pixelcnn++} to ensure the ordering is never violated.

Generating pixels one at a time lets PixelCNN solve the problem of sampling from a joint distribution. If there are two possibilities for a group of pixels (say: green light or red light) the first pixel in that group to be generated will output an appropriate probability distribution. Once we sample it and feed it as input, all the other pixels are generated \emph{conditional} on the already-generated pixel, and so can safely output 100\% probability of red or green, as appropriate (Figure \ref{lightsideal}). The network thus never needs to output a blurry image that averages between multiple possibilities, and it can capture dependencies among pixels. It also avoids the problem of mode collapse because (unlike the basic GAN) the network is penalised for failing to generate an image in the training set. Finally, because the model outputs a probability distribution at each step, it is possible to exactly calculate the model's \emph{log likelihood} -- the exact probability that the model will generate any particular image, typically calculated over the test set. For other models like VAEs and GANs, log likelihood can only be estimated with an expensive sampling procedure \cite{likelihoodestimation}, and as a result other metrics are used to evaluate their performance \cite{ganmetrics}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_ideal.png}
  \caption{The output of a PixelCNN. It generates the green dimension first, and outputs a $50/50$ chance of the green light being on or off. It then looks at our choice before generating the red dimension. If we choose \emph{on}, it generates a 100\% probability of the red light being off, and vice-versa. It thus captures the full joint distribution, including dependencies among pixels.}
  \label{lightsideal}
\end{figure}

\section{Motivation}

The PixelCNN model has three major flaws that I attempt to address. First, generation takes time linear in the number of pixels, since each pixel must be generated to be fed as input to the next pixel. Second, it has an aesthetically displeasing asymmetry. By default, PixelCNN generates pixels from top-left to bottom-right. As a result, PixelCNN can convincingly fill-in the bottom half of an image given the top half (the examples usually displayed in papers \cite{pixelcnn1,??,??}) but not vice-versa. Third, the model cannot \emph{correct} pixels that are only slightly wrong, and cannot infer which pixels in an image are wrong -- it can \nquote{fill-in} missing parts of an image only if the location of the missing region is known.

The first and second flaws (slow generation, asymmetry) are closely related. They both stem from defining an order on the pixels, and requiring each pixel to depend (only) on the preceding pixels. This is a nice idea, since it allows for an \emph{exact} factorisation of the conditional probability of an image\footnote{PixelCNN factorises the joint probability $p(\vect{x})=p(x_0,\dots,x_n)$ of the image $\vect{x}$ as $p(x_0)p(x_1\mid x_0)\dots p(x_n\mid x_0,\dots,x_{n-1})$} \cite{pixelcnn1}. But it comes with the aforementioned costs. Although the bottom-left pixel can depend on the values on all the others, the top-left pixel can only ever be generated independently, even if we are trying to fill it in and the rest of the image is available. And, of course, pixels have to be generated one at a time.

In theory, we could fill in the early pixels by performing Bayesian inference -- given the later pixels in the image, what are the most likely early pixels that could have produced them? However, such inference is computationally intractable. In fact, we use PixelCNN precisely to approximate the ideal Bayesian computation of the conditional likelihoods of all the pixels in the image. Unfortunately, the model is not easily invertible.

The need to maintain a strict ordering of the pixels and prevent information from travelling in the wrong dimensions also makes the model complicated. It is difficult to adapt for a version with any kind of pooling operation, and an image with multiple channels (e.g. red, green, blue) requires all the filters in every layer to be arbitrarily segmented according to which channel information they can access \cite{pixelcnn2}. This complexity makes the model harder to implement, makes it easier to introduce bugs, and makes it harder to experiment with variations on the basic architecture.

What is the alternative? What would it mean if pixels did not have a fixed order? In that case, a generated pixel could depend on the values of \nquote{future} pixels that have not been generated yet i.e. the model is no longer \nquote{causal}. This seems like a contradiction, but what if we fed some other value as a placeholder for missing pixels? In the simplest possible case, suppose we feed noise\footnote{I suggest using noise instead of zeros, because a region of zeros could also represent an actual black area. However, I have not tested whether this approach fails.}. Then, after the first forward pass, each pixel is generated using only noise as input. But we can do another pass, and now each pixel receives the first generated values as input. On the second pass, we hope that pixels will now be generated to conform to the choices made in the first pass. This procedure can be iterated a few (constant) number of times, and we can hope that after a few passes the generated image will converge to something reasonable\footnote{See section \ref{markovchain} for a discussion of why a constant number of iterations should be sufficient.}. This procedure is not theoretically \hl{analysis} guaranteed to converge in constant time, or at all, but in this work I show empirically that it does work.

The approach of generating all the pixels in parallel and replacing missing ones with noise has another surprising benefit. It also addresses the third issue (the need to explicitly mark missing pixels). The resulting model simply takes one image and outputs another. The input image can be noise, if we want to generate something from scratch. But it doesn't have to be. We can also feed a noisy, occluded, or otherwise corrupted version of an image, and try to generate a clean one.

\subsection{Causality}

Because my model violates causality by allowing pixels to be influenced by others that have not been generated, I dub it \nquote{noncausal} PixelCNN. The name draws an analogy to classic noncausal autoregressive models \cite{??,??}. The idea does not apply only to images. In an image, the unnaturalness of defining an order on the pixels is clear. However, the same noncausal approach also applies to text, music, video, and speech. At first glance, strict causality makes more sense for these media, which have a time dimension. Surely it makes sense to have future time steps in an audio stream depend on past ones, and not vice versa? But the fact that we have a time dimension misleads us. When composing speech in real life, we do not always proceed strictly linearly. We may know the conclusion of a paragraph before we have written the middle. Similarly, a composer does not have to write a piece of music in linear order, nor must a director film the scenes of a movie so. The \nquote{past} parts of these media can depend on the \nquote{future} parts. \hl{bidirectional RNN} \cite{??}. It makes sense to violate causality when generating these also, and I hypothesise that the results may have more sophisticated structure, by allowing the model to adjust the beginning to suit the end.

Violating causality even makes sense in the classic application of autoregressive models: the stock market \cite{??}. Here the causality assumption seems genuinely warranted -- certainly, the actual future values of the stock market cannot \emph{actually} influence past ones (unlike speech, where the whole thing may be composed before it is uttered, and so the end of a sentence really can influence the start). However, noncausal models can still be useful. There are at least two reasons for this. First, there are many hidden variables affecting the stock market that are not fed into a model that only takes stock prices as input. These hidden variables affect the future also, and thus taking future values as input can effectively give the model more insight into these variables \cite{??}. Because past stock prices do not capture all available information about the world, future prices can give extra information even though current prices do not literally depend on them. Second, stock prices depend a great deal on traders' \emph{predictions} of future prices \cite{??}. Although these predictions don't literally depend on the actual future prices, allowing a model to see the true future prices makes it easier for the model to approximate the predictions that traders make \cite{??}.

My model can be viewed as a version of PixelCNN that violates causality. My preferred variation is one that discards the complexity of PixelCNN and replaces it with a simple, symmetric, convolutional network. However, it is possible to achieve something similar with the PixelCNN architecture. When generating an image, we can simply run all of PixelCNN in parallel, replacing unavailable pixels with noise. \hl{I have tested this approach also}.

\subsection{Training}

The most difficult part of the noncausal model is training it. I think finding an optimal training approach remains important future work. The training procedure for PixelCNN no longer applies. PixelCNN is trained using example images. The model is trained to output the correct value for each pixel (or, more precisely, to minimise the cross-entropy between the predicted and actual distribution of pixel values). Because each pixel only gets previous pixels as input, this results in a model that captures the conditional probability distribution of each pixel given the previous ones. This trick will not work for the noncausal variant. Noncausal PixelCNN cannot prevent a pixel from depending on itself. Thus, if we attempt to use the same training procedure, the model can simply learn the identity, and will be useless for generation or denoising.

Even if we could prevent a pixel from depending on itself, this would not be sufficient. Consider an image where two particular pixels are always the same. The model can learn to read the value of both pixels from the other one, without learning the actual distribution of those values conditional on the rest of the image. PixelCNN solves this problem by enforcing an order on the pixels. Whichever of the two pixels comes second will always be copied from the first, but the first has to be predicted without knowledge of itself or of the second. \hl{diagram}

My preliminary approach to solving this is to force the network to denoise images. I feed the network noisy (and otherwise corrupted) versions of images, and train it to output the originals. There is room for training to reproduce an image from itself, since we want the network to act as the identity on uncorrupted natural images. However, I hypothesise that the uncorrupted parts of the images in the denoising task are enough for that. \hl{I also trained the network to produce images from pure noise, in a first-order approximation to the PixelCNN approach of learning to generate images from scratch.}

At test time, noncausal PixelCNN performs multiple forward passes on an image. The number of passes is a hyperparameter. This raises the question of how many forward passes to do during training. I do not attempt to propagate gradients back through the sampling step. \hl{discussion of how doing so would damage the network even if the discontinuity was not a problem.} Thus, we can treat each forward pass as an independent iteration, with some input (either raw data, or the output of a previous pass) and some desired ground-truth output. We have to take care with how exactly we train -- \hl{suppose we start with pure noise}. After one iteration, the network should output a probability distribution over plausible digits. The network should be invariant to the details of the initial noise, so here we can train the network to approximate the overall distribution over digits by pairing random noise with random instances of the training set. However, if we perform a sampling operation and then another forward pass, the network now gets something nonrandom as input. And the optimal output given that input is \emph{not} a uniform distribution of possible digits. Thus, it is not appropriate to train the network on more than one iteration given pure noise as input. \hl{diagram?} Given a corrupted image (noisy or occluded) on the other hand, it makes sense to train on multiple iterations, since the true image is the actual denoised or completed version of the input. It makes sense to train the network to not only produce the true image after one iteration, but also to produce the true image given its output after one iteration. I chose to train on two iterations, thus hoping to capture both the \nquote{improve the noisy image} and the \nquote{keep improving on the network's own output} behaviours. Despite training on only two iterations, the network can be run for more iterations at test time.
  
\section{Alternative Views}

Noncausal PixelCNN is conceptually a simplification of PixelCNN that removes the ordering over pixels. However, the model can be understood from other perspectives.

\subsection{Markov Chain} \label{markovchain}

\hl{TODO}

\hl{Include a discussion of why a constant number of iterations should be sufficient}

\subsection{Denoising Autoencoder}

Due to my chosen training method, noncausal PixelCNN acts as a kind of denoising autoencoder \cite{denoisingautoencoder, stackeddenoising} -- it takes noisy images as input and is trained to produce the true images as output. My model does not have a bottleneck, which explains why a pure (not denoising) version would not work and would simply learn the identity. Also unlike an autoencoder, the model's output is a probability distribution from which we can sample. Finally, the model is intended to be applied to an input iteratively, improving the result each time. \hl{I'm sure someone has iterated an autoencoder before.}

\subsection{Combination of VAE and PixelCNN}

When generating images, PixelCNN has a single source of randomness: the sampling procedure applied to the output to get the final value for each pixel. A variational autoencoder \cite{vae} has a different source of randomness: the latent vector. Noncausal PixelCNN actually has both: generating an image requires first inputting noise, and then performing multiple iterations, with a sampling step each time. The input noise is akin to the latent vector in a VAE. I think having two sources of randomness is an undesirable property, and so I train the network to ignore the details of noise. However, maybe it is possible to find a variation where both sampling and input noise are meaningful in some way.

\subsection{Projection}

Noncausal PixelCNN can be thought of as a projection operator into the natural image manifold. A true projection operator is a transformation that is \emph{idempotent}: that is, applying it more than once makes no further changes. Thus, a projection defines some manifold over which it has no effect, while all inputs outside the manifold are mapped to somewhere in the manifold. \hl{diagram}. Noncausal PixelCNN is not an ideal projection in that sense, because applying it multiple times continues to improve the result. However, it performs a similar operation of taking inputs that are not natural images, and bringing them closer to the nearest natural image. Ideally, noncausal PixelCNN should act as the identity on images that are already valid. Images that are corrupted may have multiple plausible true versions. In this case, noncausal PixelCNN chooses one of the versions at random. But since this operation requires considering dependencies among pixels, it takes the model multiple iterations to choose a point on the manifold.

Thinking about noncausal PixelCNN as a projection also reveals why the model might be suitable for performing multiple different decorruption tasks. Denoising and completing missing parts of an image are both examples of taking something that is not a natural image, and finding the \nquote{nearest} natural image (though not with the obvious L2 norm as a metric, but rather some more subtle \nquote{perceptual} distance) \cite{imageinpainting}. A network that has learnt this operation should be able to do all of these tasks, and more (e.g. deblurring, upsampling, etc.). If the projection is done in an appropriate space, using perceptual distance, then it might even be able to do some more complex tasks, like elaborating on a rough sketch. Noncausal PixelCNN as defined here is not ready for that, but I see it as interesting future work.

\subsection{Recurrent CNN}

Since noncausal PixelCNN receives its own outputs as input, it can be thought of as a recurrent neural network \cite{??}. Recurrent convolutional neural networks have been used before \cite{??,??}, but the recurrence is usually within each layer rather than connecting the output back to the input. Furthermore, unlike a typical RNN, there is a discontinuous sampling step and we do not propagate gradients back through multiple time steps.

\subsection{Neural Field} \label{neuralfields}

Neurons in multiple areas of the human brain are known to be organised in a particular fashion. First, the meaning or output space of that brain region is somehow mapped to the physical neuron space \cite{??} (for example, different locations in the brain region can represent different parts of the visual field \cite{??}, or different frequencies of sound \cite{??}, or different parts of the body \cite{??}, or different targets for a reaching action \cite{??}, or different types of action \cite{??}). Second, the strong activation of neurons in one part of the region represents a decision \cite{??} (for example, to reach in a particular direction \cite{??} or pronounce a particular syllable \cite{??}). Third, when a decision needs to be made, the region starts out with diffuse activations all over, and then converges to a single sharp peak after some time \cite{??}. This process is faster when the decision is easy, and slower when it is hard \cite{??} (because the options are very similar \cite{??}). The convergence process is influenced both by connections within the region (e.g. neurons for very different actions will inhibit each other, expressing the constraint that two different actions cannot be simultaneously chosen \cite{??}) and by connections into the region from other brain regions that have some information to contribute \cite{??}. \hl{I believe this applies to perception as well as action, where a stable percept is formed after some time, but need to check}

Work in computational neuroscience typically models the simple case where the task that a region needs to do is to choose the highest of two incoming noisy signals \cite{decisions1,decisions2}. This reproduces the basic observation that difficult decisions take longer -- it takes more time to be sure which of two noisy signals is larger if they are close together. However, the real brain regions can probably perform more complex computations.

In particular, I notice a parallel to noncausal PixelCNN -- we start with some noisy input, and slowly converge to a cleaner version. More noise makes the decision take longer, as does an input with multiple plausible ground truths. We can think of the \nquote{image} that is both input and output to the network as a group of neurons that slowly converge to a particular configuration, with the other layers of the network representing a complex web of inhibitory and excitatory connections between them. Alternatively, other layers could represent other brain regions, which suggests a modification to the architecture: adding recurrent connections of some kind within each layer, so that each layer of the network can converge to some representation over time. And the layers could be connected in some other way than a purely linear network, like by adding a web of skip connections \cite{denselyconnected}.

\section{Literature Review}

\subsection{Generative Models}

\subsubsection{PixelCNN}
\cite{pixelcnn1} introduced the PixelCNN family of autoregressive generative models. In this first paper, they presented both a convolutional and a recurrent variant, and found that the recurrent one performed better. PixelRNN connects the pixels with a grid of recurrent network units. It has the major downside that information from distant pixels has to cross many units, and the gradients are therefore weak. An idea that appeared here but was discarded in later variants was to do generation in multiple steps -- first generate a low-resolution image, which can then guide the high-resolution generator by (sort of) allowing it to peek ahead as well as look back. In \cite{pixelcnn2} they argued that the advantage of the recurrent variant came down to its more sophisticated nonlinearities, and so they introduced the standard version of PixelCNN, which is convolutional but has a complex multiplicative gate nonlinearity instead of the more traditional ReLU units. Further work has focused on extending this basic model. \cite{pixelcnn3} Introduce location-dependent conditioning that allows the model to generate images with a predefined structure (e.g. \emph{these} pixels should belong to a \emph{person}).

\cite{pixelcnn++} make several improvements to the model (dubbed PixelCNN+), most notably a change to the output units. Basic PixelCNN uses a 256-way softmax, which allows it to learn arbitrary distributions over output pixels, but prevents it from automatically recognising the natural \nquote{similarity} between pixels of similar colours. \cite{pixelcnn++} replace the softmax with a logistic mixture that depends on a much smaller number of parameters (5 or so) and implicitly encodes the assumption that similar pixel values can substitute for each other. They also simplify the dependencies among channels of the same pixel. Basic PixelCNN requires that the filters at each layer be segregated according to which channel they are responsible for (so that they can receive information from previous channels, but not later ones). PixelCNN+ instead makes all filters equal, and simply requires that the channels in the final output have a linear dependency on previously generated channels.

\cite{auxiliary} look into how to speed up PixelCNN generation. They formalise the general idea of first generating a simpler (e.g. scaled down) version of the target, and then using that as input for the final generation step. They propose generating a series of increasingly larger resolution images, each taking the previous as input. Crucially, they suggest that it should be possible to have most of the model complexity concentrated in the component that depends only on the lower-resolution image (which only needs to be evaluated once per resolution), leaving only a very simple model in the autoregressive part. Thus, although the result is still $O(n)$, it should be significantly faster than the single-shot PixelCNN. \cite{multiscale} take this idea further and actually improve the time to $O(\log N)$ by generating pixels in four interleaved groups (top-left, top-right, bottom-left, bottom-right of each 2 by 2 block), ignoring all dependencies within each group, and generating the first group recursively from a lower-resolution image. For video, they generate individual frames in $O(1)$ by conditioning the first group of pixels only on previous frames (and not on each other).

WaveNet \cite{wavenet} is a very successful application of the PixelCNN architecture to a different modality: sound waveforms. The model can generate speech or music. This paper also introduced the \emph{dilated convolution} (a convolution that takes every $n$'th pixel as input instead of a contiguous block) for increasing the receptive field of output units to allow them to depend on very distant data points. \cite{videopixel} extend the idea to video. They use ordinary PixelCNN within each frame of a video, but add a recurrent component to pool information across frames. This paper also introduced some even \emph{more} complicated nonlinearities which they call \nquote{multiplicative units}. ByteNet \cite{bytenet} applied the idea to the translation of text. They use dilated convolutions to encode input text into a (variable-length) intermediate representation, and then PixelCNN units to decode the representation (one token at a time) into the target language. \cite{quantile} experiment with a different type of output: one that takes a \emph{quantile} between 0 and 1 as input and then \emph{deterministically} generates an output (i.e. the source of randomness is inserted as input to the network rather than being a part of a sampling process at the end). They argue that this method better captures the perceptual details of an image. \cite{superres} apply the architecture to upscaling images, which has the interesting effect that the network doesn't just get previously generated pixels as input -- it also gets low-resolution versions of the rest of the image, a bit like the multi-stage process in \cite{pixelcnn1}. \cite{pixelsnail} extend PixelCNN's ability to make use of distant data by adding an attention mechanism. \cite{fbtranslate} improve on the translation results of \cite{bytenet} by adding an attention mechanism.

\subsubsection{GAN}

Since their introduction by \cite{gan}, GANs have exploded in popularity due to their uncanny ability to generate more realistic samples than were possible before. Some milestones in the field include the following. \cite{imagefromcaption} generate images conditional on text captions and introduce an attention mechanism. \cite{unsupervisedgan} use a GAN for unsupervised learning. \cite{vaegan} combine a VAE with an adversarial training mechanism. \cite{structurestylegan} separate the task of image generation into that of generating the structure and style (both with GANs). \cite{texttoimagegan} extend text-to-image synthesis to the generation of images from detailed descriptions. \cite{preferredinputs} use a GAN to generate inputs that maximally activate a particular neuron in a network while making sure the images remain interpretable (i.e. they stay on the natural image manifold). \cite{manifoldmanipulation} use GANs to constrain edits to lie on the natural image manifold, thus making it easy to edit images without destroying their naturalness. \cite{imagetoimagegan} use a GAN to convert images to other images (e.g. recolour, change the style, etc.). \cite{cyclegan} improve on this by forcing the GAN to simultaneously learn the inverse transformation. \cite{stackgan} generate $256\times 256$ images by first generating a lower-resolution version and then feeding that as input to guide the higher-resolution generator. \cite{progressivegrowing} generate $1024\times 1024$ images by training the networks one layer at a time, while progressively adding larger layers and increasing the resolution of the samples. \cite{vaeganinterpolation} use a GAN to improve an autoencoder's ability to interpolate between training data points. \cite{largegan} train the largest GAN to date, on $128\times 128$ ImageNet.

GANs suffer from well-known issues of unstable training and mode collapse \cite{gantechniques}. Many groups have proposed solutions to these problems. \cite{gantechniques} allow the discriminator to peek at other examples in a minibatch to detect if they are too similar to the current example. \cite{energygan} modify the loss to encourage orthogonal minibatches. \cite{unrolledgan} improve the loss by training the generator to optimise not just the current discriminator, but the \emph{predicted} discriminator as it will look like after some number of gradient descent steps. \cite{multiagentgan} address mode collapse using multiple generators that are encouraged to differ from each other. \cite{progressivegrowing} encourage diversity by feeding the variance of each minibatch as an extra input to the discriminator, allowing it to detect when the generator produces samples with too little variety. \cite{wgan} is a major step, introducing a new training procedure that is much less likely to suffer from vanishing gradients or to mode collapse. The \nquote{Wasserstein GAN} replaces the discriminator (whose goal is to ultimately assign 1 to all true data and 0 to all generated data) with a critic whose goal is to assign a score that varies continuously from the true data points least represented in the generated set to the generated points least similar to the true set. This approach means the critic always has useful gradients for the generator to use, and discourages mode collapse since the generator can always improve by reducing the distance to the most neglected true data. \cite{improvedwgans} improve convergence of Wasserstein GANs by adding the magnitude of the critic's gradient to the loss.

Quantitatively evaluating GANs is harder than autoregressive models because the log-likelihood is not easily calculated. For any given image, the output of PixelCNN is a probability distribution over each pixel given previous pixels. Thus, we can compute the probability that the model would have generated any particular pixel, and we can multiply the probabilities (or add their logs) to find the probability of generating the whole image. Thus we can evaluate the \nquote{log-likelihood} that a PixelCNN model would have generated the images in a test set. But a GAN does not output probabilities, so it is less clear how to evaluate it. \cite{gantournament} propose comparing GANs with a tournament between their respective generators and discriminators. \cite{likelihoodestimation} derive a sampling-based approximation to the log-likelihood for GANs and VAEs. \cite{ganmetrics} compare multiple metrics currently in use (though not \cite{likelihoodestimation}'s method), and recommend \nquote{maximum mean discrepancy} \cite{mmd} as the one that is both computationally tractable and best captures the desiderata of a good GAN. Although their paper is focused on GANs, it really applies to any generative model, since all they look at are metrics that compare two sets of samples -- the test set and the generated data.

\subsubsection{VAE}

The variational autoencoder was introduced by \cite{vae} and the training procedure improved upon by \cite{vae2}. \cite{drawvae} used a VAE as part of a larger network for generating images. \cite{vaeflows} further improve on the training procedure. \cite{adversarialvae} use an adversarial network instead of a hand-crafted loss to enforce the correct distribution of latent vectors. \cite{vaegan} combine a VAE with an adversarial training mechanism to improve upon the reconstruction quality component of the loss. \cite{vaecompression} use a VAE to lossily compress data and then reconstruct the details with a generative model. \cite{vaeiaf} incorporate some ideas from PixelCNN into the VAE framework. \cite{vaelossy} also combine a VAE with PixelCNN, to encourage the model to learn more useful features for downstream learning. \cite{pixelvae} use PixelCNN for the decoding stage of a VAE to improve its ability to capture fine details. \cite{vaehieararchy} find a way to make the VAE hierarchical, with multiple levels of latent vectors. \cite{vaeunderstanding} develop a generalisation of the VAE training procedure and analyse reasons for blurry samples. \cite{vqvae} introduce a quantised version of the VAE that has discrete outputs.

\subsection{Noncausal Autoregression}

\hl{- Review of noncausal autoregression:

  - economics and the old autoregressive models
  
  - (un)importance of causality, for stock market data (most causal), audio/text/video (less causal), images (least causal).
  
  - bidirectional RNN}
  
\subsection{Monte Carlo Generation}

\subsection{Neural Fields}

\hl{- Review of neural fields in neuroscience and in AI}

\subsection{Recurrent Convolutional Networks}

\hl{- Review of recurrent convolutional networks}
\cite{videopixel}

\section{Methods}

I performed experiments with the MNIST database of handwritten digits \cite{mnist}. Specifically, I used a binarised MNIST \cite{binarisedmnist}, where each pixel is only black or white, with no grays, like the original PixelCNN paper \cite{pixelcnn1}. I trained three models: the ordinary PixelCNN, my noncausal PixelCNN, and a version of PixelCNN that has the normal architecture but is trained with the same procedure as the noncausal version (which I will call denoising PixelCNN). I tried to use the same hyperparameters for all three, though the architectures are slightly different and not directly comparable. Once trained, I evaluated the networks in two ways: by visual inspection of generated images and with the MMD metric \cite{mmd,ganmetrics}.

Source code for all the experiments is available at \url{https://github.com/dmitry-brizhinev/honours-code}

\subsection{PixelCNN}

I used a basic version of PixelCNN, similar to the second paper \cite{pixelcnn2}. Figure \ref{pixelcnn} shows the architecture. There was a vertical and a horizontal stack (to avoid a blind spot \cite{pixelcnn2}). The vertical stack used shifting to preserve the causal structure. I used $4\times 5$ convolutions because in combination with the horizontal stack this made the total number of parameters roughly equivalent to the noncausal variant. The horizontal stack also used shifting, with $1\times 5$ convolutions. All the convolutions were shifted (see Figure \ref{convolutions}). Each layer included a residual connection and a skip connection to the end. \hl{explain exactly which papers I consulted to get this architecture -- it's a combination of one that had a better diagram and another that had a simpler architecture}. \hl{Also mention how I only have one channel, which simplifies things?} The final layers were a fully-connected one and a softmax output (with two output neurons per pixel, for white and black).

\begin{figure}
  \centering
  \vspace{-3cm}
    \begin{subfigure}{0.45\columnwidth}
    	\addtocounter{subfigure}{2}
        \centering
        \caption{Last convolutional layer}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_last_layer.png} 
        \label{pixelcnn:last}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Final fully-connected layers}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_end_layer.png} 
        \label{pixelcnn:end}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    \addtocounter{subfigure}{-4}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{First layer}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_first_layer.png} 
        \label{pixelcnn:first}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Intermediate layers (repeated $13\times$)}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_middle_layer.png} 
        \label{pixelcnn:middle}
    \end{subfigure}
  \caption{The architecture I used for PixelCNN. Data flows from bottom to top. Numbers next to the lines represent the number of channels per pixel (the number of pixels stays constant). Dark blue boxes represent shifts. Light green boxes represent convolutions with bias terms, and the two light blue boxes (on the residual connections in the first and last layer) represent convolutions without bias terms. Yellow circles represent elementwise nonlinearities. Red circles represent binary elementwise operations. Blue diamonds represent dividing the channels into two streams.}
  \label{pixelcnn}
\end{figure}

\begin{figure}
  \centering
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \caption{Normal $5\times 5$ convolution}
        \includegraphics[width=\linewidth]{../../Diagrams/convolution_normal.png} 
        \label{convolutions:normal}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \caption{Shifted $4\times 5$ convolution}
        \includegraphics[width=\linewidth]{../../Diagrams/convolution_45.png} 
        \label{convolutions:45}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \caption{Shifted $1\times 5$ convolution}
        \includegraphics[width=\linewidth]{../../Diagrams/convolution_15.png} 
        \label{convolutions:15}
    \end{subfigure}
  \caption{An illustration of the shifted convolutions used for PixelCNN. (\subref{convolutions:normal}) In a normal convolution, the middle pixel in the next layer receives the result of convolving the $5\times 5$ region with a filter. (\subref{convolutions:45}) In this shifted convolution, the bottom pixel receives the result. This ensures that no pixel in any layer ever receives any information about pixels below it. (\subref{convolutions:15}) The right pixel receives the result. This ensures that no pixel in any layer ever receives any information about pixels to the right of it.}
  \label{convolutions}
\end{figure}

I used 15 convolutional layers, each the same dimension as the image (convolutions were zero-padded at the edges) with 16 feature filters per layer. I think of the filter size as being $5\times 5$, though as described above this is split between the horizontal and vertical stacks. The final fully connected layers used 64 features per layer. When sampling, I used a temperature of $1/1.05$\footnote{See section \ref{temperature} or \cite{pixelcnn2} for an explanation of the sampling temperature}. I trained for 200000 iterations, with 32 instances in each minibatch. I used RMSProp with a fixed learning rate of $1\mathrm{e}{-4}$. \hl{explain the sources of all of these}.

The training procedure is to perform a forward pass with a training example as input, and then to optimise the cross-entropy between the softmax outputs and the ground truth (the same training example).

\subsection{Noncausal PixelCNN}

The structure of noncausal PixelCNN is much simpler, being simply a stack of (residual \cite{resnet}) convolutional layers, with no shifting or masking required (Figure \ref{noncausal}). I used the same gates as PixelCNN, and included residual and skip connections. I attempted to match the hyperparameters of PixelCNN.

\begin{figure}
  \centering
  \vspace{-3cm}
    \begin{subfigure}{0.35\columnwidth}
    	\addtocounter{subfigure}{2}
        \centering
        \caption{Last convolutional layer}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_last_layer.png} 
        \label{noncausal:last}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Final fully-connected layers}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_end_layer.png} 
        \label{noncausal:end}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    \addtocounter{subfigure}{-4}
    \begin{subfigure}{0.35\columnwidth}
        \centering
        \caption{First layer}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_first_layer.png} 
        \label{noncausal:first}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\columnwidth}
        \centering
        \caption{Intermediate layers (repeated $13\times$)}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_middle_layer.png} 
        \label{noncausal:middle}
    \end{subfigure}
  \caption{The architecture I used for my noncausal PixelCNN. Data flows from bottom to top. Numbers next to the lines represent the number of channels per pixel (the number of pixels stays constant). Light green boxes represent convolutions with bias terms, and the two light blue boxes (on the residual connections in the first and last layer) represent convolutions without bias terms. Yellow circles represent elementwise nonlinearities. Red circles represent binary elementwise operations. Blue diamonds represent dividing the channels into two streams.}
  \label{noncausal}
\end{figure}

I used 15 convolutional layers, each the same dimension as the image (convolutions were zero-padded at the edges) with 16 feature filters per layer and a filter size of $5\times 5$. The final fully connected layers used 64 features per layer. When sampling, I used a temperature of $1/1.05$. I trained for 200000 iterations, with 32 instances in each minibatch. I used RMSProp with a fixed learning rate of $1\mathrm{e}{-4}$.

\hl{The training procedure for noncausal PixelCNN is different. There are two types of training examples: decorruption and generation. In generation examples, the input is pure noise and the desired result is to minimise cross-entropy between the softmax outputs and a particular training example. In decorruption examples, the input is a corrupted training example and the goal is to minimise cross-entropy between the softmax outputs and the uncorrupted example. Each decorruption example produces two forward passes: one using the corrupted input, and another using the output of the first as an input for a second iteration. I averaged the errors for these two, and treated them as a single training example.} \hl{diagram?} \hl{in hindsight, I shouldn't average the errors, I should treat them as two batches, because I can't easily do the same for the denoising variant: if I have time and funding I might run this training procedure again} \hl{I alternated between minibatches of 32 decorruption examples and 32 generation examples. Each MNIST image produced 5 decorruption examples: one where the top half was replaced with noise, one where the bottom half was replaced with noise, one where a random 50\% of the pixels were replaced with noise, one where a Gaussian blur was applied with $\sigma=1.0$, and one where the image was downsampled by a factor of $2\times$(using bicubic interpolation) and then upsampled again (using bilinear interpolation).} \hl{if I run this again I will rethink the blurring and scaling: first, the blurred and scaled ones look very similar; second, the result is not binary and I should binarise it like the rest of the data; third, I think the task is too easy, maybe I need to increase the strength of the blur} \hl{For the three noise-based decorruption types, and for the input noise for the generation examples, noise pixels were drawn from roughly the same distribution as the pixels in binarised MNIST: each noise pixel was black with probability 0.87 and white otherwise.}

\subsection{Denoising PixelCNN}

\hl{I haven't run this experiment yet so the details might change}

The architecture for the denoising variant was the same as ordinary PixelCNN above. All the hyperparameters were also the same.

\hl{The training procedure, however, was the same as for noncausal PixelCNN, with one alteration. Instead of averaging the errors for the two forward passes of the decorruption iterations, I treated them as separate batches. I thus alternated between two batches in a row of decorruption (first and second pass) and two batches in a row of generation.}

\section{Results}

\hl{- Results

  - Hopefully even if I don't beat PixelCNN, I will demonstrate that it's not too much worse, but runs in constant time and can do a bunch of things pixelcnn can't
  
  - Include training curves}
  
\section{Future Work}

There are many possible improvements to noncausal PixelCNN and future research directions to explore.

\subsection{Existing Architectural Improvements}

The version of PixelCNN I used as a baseline is a fairly basic one, without some improvements that have been added since. PixelCNN++ \cite{pixelcnn++} adds dropout regularisation and replaces softmax outputs with a logistic mixture that reduces the number of independent output neurons and makes it easier for the network to learn the idea that some colours are closer than others. They also add a bottleneck, which makes the architecture more like an autoencoder. The original PixelCNN team have experimented with class-conditional generation \cite{pixelcnn2}, different gates \cite{videopixel} and dilated convolutions \cite{wavenet}. All of these improvements could apply to noncausal PixelCNN also.

\subsection{Hyperparameter Search}

I have reused PixelCNN hyperparameters for the noncausal variant, but it is plausible that the optimal hyperparameters for the noncausal variant are different. A hyperparameter search is beyond my means, but might produce further improvements. Besides the hyperparameters I have explicitly mentioned in the Methods section, there are other variations that could be applied, like batch normalisation \cite{batchnormalisation}, L2 regularisation, and gradient clipping \cite{gradientclipping}.

\subsection{Other Datasets}

Just like PixelCNN has been applied to other datasets \cite{pixelcnn2,wavenet,bytenet,videopixel}, noncausal PixelCNN can be applied to them too. It would be good to see results on Imagenet \cite{imagenet} and CIFAR \cite{cifar}, and on speech, music, text, and video. In particular, I hypothesise that noncausal PixelCNN can improve on the structural properties of generated sequences (like speech) because it can adapt the beginning of a sequence to suit the end, rather than always having to compose the sequence in temporal order.

Experiments with different sizes of Imagenet (like the $32\times 32$ variant used for PixelCNN \cite{pixelcnn2}) would be needed to test how the number of iterations depends on the input size. I hypothesise that the number can remain constant (see section \ref{markovchain}), but in this work I only demonstrate that it is markedly less than $N$. It is possible that more iterations are needed to achieve convergence for larger inputs -- the scaling might be $O(\log{N})$ or even $O(N)$ (but with a much smaller constant than ordinary PixelCNN).

\subsection{Improving on the Training Procedure}

The training procedure I have used for noncausal PixelCNN is ad-hoc. Besides experimenting with different kinds of input corruption and testing hyperparameters, there is room for improvement in the general procedure itself. I am not sure what an improved procedure would look like, but I am not fully satisfied with just denoising. I think it would be good to express the desired learning objective via some combination of architecture and loss function, rather than through data augmentation as I have. \hl{augment this with discussion of monte carlo.}

\subsection{Theoretical Analysis}

In this work I have not put much thought into a detailed theoretical analysis of noncausal PixelCNN. On the contrary, my goal was to show empirically that we do not \emph{need} to maintain an exact factorisation of the probability distribution as PixelCNN provides, and that we can get away with a simpler architecture that makes little sense given available theory.

However, I expect that it is possible to put the noncausal variant on a more rigorous theoretical footing, like classical noncausal autoregressive models \cite{??}. Such an analysis might also suggest improvements to the architecture that have not occurred to me while doing largely empirical work. \hl{note that even the rough theoretical ideas like thinking about the network as a projection, thinking about neural fields, thinking about the blurring problem, have all informed the architecture I have designed}. \hl{augment this with discussion of monte carlo.}

\subsection{Experimental Architectural Improvements}

There are many possible improvements to the noncausal PixelCNN architecture that are worth trying or thinking more about.

\subsubsection{Recurrence}

As suggested by the discussion comparing the architecture to neural fields (section \ref{neuralfields}), it may be worth considering recurrent connections within each layer. Rather than traditional recurrent connections, maybe these should be sampled from probability distributions, the same way the output of the current model passes through a sampling step before being fed as input. The result would be more similar to actual neural fields, and would operate slightly differently -- rather than an operation repeatedly applied to an image, we would have to see the network more as a set of abstract representations of the image that influence each other and converge to something over time.

There may be other ways to improve the architecture with ideas from neural fields that I haven't thought of yet.

\subsubsection{Rethinking the Use of Input Noise}

In the current model, the input noise acts as a VAE-like latent vector that affects the output independently of the sampling procedure. Ideally, I feel that there should only be one canonical source of randomness in the generation procedure. It would be good to rethink the architecture and find a way to eliminate one of the two sources of randomness. I do not think the sampling procedure can be eliminated without reintroducing the problem of bluriness, but I am not certain.

I am likewise not sure how noise can be eliminated. One possibility is to add an extra channel that flags whether a pixel is a known part of the image or not -- but this reintroduces PixelCNN's weakness of needing to know which pixels are valid. Alternatively, perhaps the network can be trained to use zeros as input for missing pixels, instead of noise. My hypothesis is that this will work poorly because the network will interpret zeros as a region of the image known to be black, but perhaps I am wrong.

\subsubsection{Reducing Temperature} \label{temperature}

\hl{explain what the temperature is here}

Since we expect the output to get closer and closer to a natural image with more iterations, it might make sense to reduce the sampling temperature with each iteration -- that is, early iterations allow more randomness, while later ones force the network to settle on the most probable option.

Another way to accomplish this might be with some form of sparsity regularisation \cite{??}. The network as currently structured is a residual network \cite{resnet}. Applying sparsity regularisation to the output is thus equivalent to limiting the number of pixels that are allowed to change. \hl{I think this might not be quite true for my architecture as it stands, since the output is a probability distribution that is not expressed as a residual -- maybe the architecture needs extra thought to make this possible}. It makes sense to add such regularisation, getting stronger with each iteration, to express the fact that we expect more and more pixels to have taken their final value.

\subsubsection{Better Losses}

\hl{I've read about it now, so I need to write about Wasserstein distance}

\subsubsection{Working in the Image Manifold}

Some more ideas are suggested by the view of noncausal PixelCNN as a projection into the natural image manifold. \cite{manifoldmanipulation} use a GAN to constrain edits to an image to lie on the manifold, by using gradient descent to find a latent vector that corresponds to the image closest to the edit. Similarly, \cite{imageinpainting} use gradient descent to find a latent vector that corresponds to the on-manifold completion of an off-manifold partially occluded image. Noncausal PixelCNN could do something similar: simply apply a few iterations after an edit to \nquote{return} the result to the manifold, or apply them to an occluded image to get a plausible completion.

Viewing the operation as a quick \nquote{bring this back to the natural image manifold} suggests further ideas. For example, one can start with a rough sketch and then apply multiple iterations of noncausal PixelCNN to obtain a natural image based on the sketch. One could even interleave sketching with projection, making extra edits to move the projection in the desired direction. This procedure might work better if the network is somehow trained to understand the space of images under an appropriate metric -- we want a rough sketch to project to the object being sketched, not some other image with black pixels in the same location (as one would get if finding the nearest neighbour under the L2 norm). I am not sure how to accomplish this, but it is an intriguing possibility for future work.

A general \nquote{natural imagification} operator would be a marvellous achievement. It could simultaneously be used for denoising, completion, deblurring, upsampling, sketching, and editing. And it would represent a new kind of understanding of natural images. Noncausal PixelCNN is a first step in that direction.

\hl{Do I need a conclusion section here?}

\bibliography{thesis}
\end{document}