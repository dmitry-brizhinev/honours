
\documentclass[11pt, a4paper, openany]{book}
\usepackage{svn-multi}
\svnid{$Id$}
%\usepackage{prelim2e}
%\renewcommand{\PrelimWords}{Draft Copy \svnkw{Id}}
%%\newcommand*{\mysvnrev}{\svnrev}
\usepackage[hyperindex=true,
			bookmarks=true,
            pdftitle={}, pdfauthor={Dmitry Brizhinev},
            colorlinks=false,
            pdfborder={0 0 0},
            pagebackref=false,
            citecolor=blue,
            plainpages=false,
            pdfpagelabels,
            pagebackref=true,
            hyperfootnotes=false]{hyperref}
\usepackage[all]{hypcap}
\usepackage[palatino]{anuthesis}
\usepackage{afterpage}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{thesis}
\usepackage[square]{natbib}
\usepackage[normalem]{ulem}
\usepackage[table]{xcolor}
\usepackage{makeidx}
%\usepackage[centerlast]{caption2}
\usepackage{float}
\urlstyle{sf}
\renewcommand{\sfdefault}{uop}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beramono}

\renewcommand{\bibname}{References}

\usepackage{multirow}

\usepackage{subcaption}

\sethlcolor{cyan}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\nquote}[1]{``{#1}''}
\newcommand\cites[1]{\citeauthor{#1}'s\ [\citeyear{#1}]}


\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{
  \ifcase #1 %
    %
  \or
    (cited on page #2)%
  \else
    (cited on pages #2)%
  \fi
}

\usepackage{booktabs}
\usepackage{relsize}
\usepackage{xspace}
\DeclareGraphicsRule{*}{pdf}{*}{}

\usepackage{setspace}
\usepackage{ifthen}

\long\def\sfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}
           
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Preamble
\title{Noncausal PixelCNN}
\author{Dmitry Brizhinev}
\date{\today}

\renewcommand{\thepage}{\roman{page}}

\makeindex
\begin{document}
%\doparttoc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page
\pagestyle{empty}
\thispagestyle{empty}
%% anuthesis.sty Copyright (C) 1996, 1997 Steve Blackburn
%% Department of Computer Science, Australian National University
%%

\begin{titlepage}
  \enlargethispage{2cm}
  \begin{center}
    \makeatletter
    \Huge\textbf{\@title} \\[.4cm]
    \Huge\textbf{\thesisqualifier} \\[2.5cm]
    \huge\textbf{\@author} \\[9cm]
    \makeatother
    \LARGE A thesis submitted for the degree of \\
    Bachelor of Advanced Computing (Research and Development) (Honours) \\
    The Australian National University \\[2cm]
    \thismonth
  \end{center}
\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Here begin the preliminaries
\vspace*{7cm}
\begin{center}
  Except where otherwise indicated, this thesis is my own original
  work.
\end{center}

\vspace*{4cm}

\hspace{8cm}\makeatletter\@author\makeatother\par
\hspace{8cm}\today

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
\cleardoublepage
\pagestyle{empty}
\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgments}
I want to thank my supervisor Tom Gedeon for his advice and support throughout this project, and my family for their support throughout my studies.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract
\pagestyle{headings}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\vspace{-1em}
I introduce a modification to the generative model PixelCNN, which allows generation to complete in constant time, rather than requiring a separate forward pass for each pixel of the output. The key idea is to not enforce a strict causal order like in PixelCNN, effectively allowing generated pixels to depend on both the past and the future. The model is based on theoretical work done on Monte Carlo generation and on empirical work previously done on autoregressive models similar to PixelCNN. My version of this model does not quite match PixelCNN's performance, but the results show promise and I suspect they could be improved with better training and a hyperparameter search. The new model markedly improves the running time of PixelCNN in its core task -- generation -- and can also perform tasks that PixelCNN cannot, such as denoising and filling-in the top half of an image.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Table of contents
\pagestyle{headings}
\markboth{Contents}{Contents}
\tableofcontents
\listoffigures
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Here begins the main text
\mainmatter

%% Introduction
\chapter{Introduction}
\label{cha:intro}

Generating images is a task that demonstrates the power of deep learning, even more so than recognising them, because it requires a model capable of understanding the whole structure of the data at hand. There are several kinds of such generative models, but the one this work will focus on is an autoregressive model called PixelCNN, which generates data one value at a time by taking all previously generated values as input. It started out as an image model, but has since been applied to other domains, like language. Although training is fast (insofar as any deep network's training can be called \nquote{fast}), generation is very slow, because a new forward pass through the network must be performed for every value (i.e. it takes $O(N)$ forward passes for data of dimensionality $N$). This fundamentally cannot be parallelised since each pass depends on the values generated in previous passes. This is a major limitation on the size of images that can be feasibly generated, even with massive computing clusters. In this work, I suggest a faster variant -- by replacing values that have not yet been generated with noise, I can generate multiple values at once. The model cannot generate final data in one forward pass, but it requires only $O(1)$ such passes, which is a significant improvement.

\section{Thesis Outline}
\label{sec:outline}
In Chapter~\ref{cha:background}, I introduce the task of generating images and the various approaches that have succeeded at the task. I then introduce and motivate my new architecture. Finally, I present multiple different perspectives for understanding my architecture, thus connecting it to existing literature and providing ideas for futher improvements.

In Chapter~\ref{cha:review}, I review relevant literature. I cover other generative models, and other work in fields that I believe are related to my architecture.

In Chapter~\ref{cha:methods}, I describe the experiments I performed, including a detailed description of the model architectures and their hyperparameters. This chapter also includes a link to the source code for all my experiments.

In Chapter~\ref{cha:result}, I present and discuss the results of my experiments.

Finally, in Chapter~\ref{cha:conclusion}, I end with concluding remarks and propose improvements and future directions for research.

%% Chapters
\chapter{Background}
\label{cha:background}
Before I introduce my improvements, I explain the motivation behind PixelCNN and why the $O(N)$ generation procedure was reasonable in first place. The core difficulty with generating images is that a model needs to capture the full joint probability distribution of all the pixels -- every pixel can potentially depend on every other in complex ways (ones that cannot be modelled by multivariate Gaussians). A neural network must be able to capture these relationships, and to communicate them. The different classes of generative models can be distinguished by the ways in which they enable the network to do this. Once this is explained in more detail, I will be able to introduce and motivate my idea. Finally, I think it is valuable to consider my proposed new architecture from multiple perspectives. Although I have designed it as a modification of PixelCNN, a similar architecture could be arrived at from completely different directions -- as a denoising Markov chain or as a deep neural field. Considering these perspectives illuminates the connections between different approaches to generation and between different fields of AI. It also suggests new ideas for the architecture (explored in more detail in Chapter~\ref{cha:conclusion}) that would have been much less obvious when thinking purely from the perspective of a modification to PixelCNN.

\section{Generating Images}

The generation of synthetic image data is an important step on the way to complete machine understanding of images (and the same applies to other media). In the deep learning field, there are currently three broad popular approaches to the generation of images: variational autoencoders (VAEs) \citep{vae}, generative adversarial networks (GANs) \citep{gan}, and autoregressive models like PixelCNN \citep{pixelcnn2}.

\subsection{The natural image manifold}

All three approaches attempt to solve the problem of how a neural network can both \emph{represent} the set of \nquote{natural} images, and to \emph{sample} from it. It is common to think of natural images as defining a manifold (continuous subset) in the space of all possible images \citep{manifoldmanipulation,imageinpainting,manifoldmixup}. Importantly, this manifold is not convex -- the average of two natural images is often not a natural image itself (see Figure~\ref{manifold}). A network trained to generate natural images implicitly defines the shape of a manifold \citep{manifoldmanipulation}, which we hope is as close as possible to the \nquote{true} natural image manifold. Thinking in terms of the natural image manifold is fruitful -- for example, \citet{manifoldmanipulation} apply this idea to image editing, using a GAN to constrain edits to remain within the manifold -- that is, unlike ordinary per-pixel edits which quickly make the image look doctored and unrealistic, their system tries to approximate what the user wants while ensuring that the image remains natural at all times.

\begin{figure}
	\centering	
    \begin{subfigure}{\columnwidth}
        \centering
        \caption{Pixel space}
        \includegraphics[width=\linewidth]{../../Diagrams/manifold_pixel.png}
        \label{manifold:pixel}
    \end{subfigure}
    \caption[Simplified schematic of the natural image manifold]{A simplified schematic of the natural image manifold. (\subref{manifold:pixel}) The manifold in pixel space (i.e. each dimension is the value of a pixel). In reality this would be a $3N$ dimensional space for images with $N$ pixels of 3 channels each. Crucially, the manifold is not convex -- the average of a cat and dog image in pixel space is not a natural image. (\subref{manifold:perceptual}) The same manifold in a different space, something where the dimensions are perceptually meaningful. In this case, if the dimensions are appropriate, the average of a cat and a dog image might be an image of something that looks both cat- and dog-like. We generally hope that the dimensions of the latent vector of a VAE or a GAN will form such a perceptual space.}
    \label{manifold}
\end{figure}
    
\begin{figure}
\ContinuedFloat
    \centering	
    \begin{subfigure}{\columnwidth}
        \centering
        \caption{Perceptual space}
        \includegraphics[width=\linewidth]{../../Diagrams/manifold_perceptual.png} 
        \label{manifold:perceptual}
    \end{subfigure}
\end{figure}


\subsection{Sampling from a joint distribution}

\begin{figure}
  \centering
  
      \begin{subfigure}{0.7\columnwidth}
        \centering
        \caption{Traffic light dataset}
        \includegraphics[width=\linewidth]{../../Diagrams/lights_intro.png} 
        \label{trafficlights:example}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Data space}
        \includegraphics[width=\linewidth]{../../Diagrams/lights_example.png} 
        \label{trafficlights:axes}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Traffic light manifold}
        \includegraphics[width=\linewidth]{../../Diagrams/lights_plain.png} 
        \label{trafficlights:manifold}
    \end{subfigure}
  \caption[Traffic light example]{I will use this example for the rest of this section. (\subref{trafficlights:example}) Imagine that our dataset is these traffic lights. When we generate traffic lights, they must either have a green light or a red light. They should not have neither, both, or an average of the two. (\subref{trafficlights:axes}) With only two dimensions of variation -- the brightness of the green and red lights -- we can represent the complete space of possible data points. (\subref{trafficlights:manifold}) The valid regions are the top-left (only green) and bottom-right (only red), so the \nquote{traffic light manifold} is those two areas.}
  \label{trafficlights}
\end{figure}

Consider the simplest possible approach to generating images. We train a deterministic, feedforward network with a fixed input to produce the desired images (Figure~\ref{trafficlights}) as output (say, with L2 loss). The problem here is clear: the network can only ever produce one output, and so the optimal strategy (to minimise L2 loss) is to output the mean of all the images in the training set (Figure~\ref{lightsmean}). This network fails to learn much about natural images.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_mean.png}
  \caption[Output of a deterministic neural network]{The output of a deterministic neural network. It can only choose one point. If the network tries to optimise L2 loss, the best place is the average of the valid points. This is not a good generator. In practice, the result is blurry images.}
  \label{lightsmean}
\end{figure}

Now suppose the deterministic network can receive a random seed as input. For each training example, we pair one of our images with a random input seed. But now the network still has no choice but to output the mean: the seeds provide it with no information about the images it needs to generate. If we instead associate each image with a unique seed, and make sure the same seed appears each time the same image is seen, the network now has a chance, but we deprive it of the ability to learn any structure in the image space -- it has to memorise the mapping from seeds to images, and will not generalise (Figure~\ref{randompoints}).

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_randompoints.png}
  \caption[Output of a deterministic network with random values as input]{The output of a deterministic network that gets random values as input, where each value is paired to a fixed output. It could learn to output the correct data by memorising it, but it cannot learn any of the structure, since the mapping from input to output is random. If we feed the network a new input value it has not seen before, there is no reason to expect a meaningful output.}
  \label{randompoints}
\end{figure}

Finally, suppose we let the network be probabilistic. Instead of outputting images, the network outputs probability distributions over each pixel (with cross-entropy loss). We still face a problem here: the network no longer has to output the mean, but the best it can do is to learn the distribution of each pixel independently (Figure~\ref{independent}). The network cannot learn even as simple a relationship as that two particular neighbouring pixels are always the same colour. If each part of the traffic light can be on or off with 50\% probability, the best it can do is predict a 50\% probability of each possibility for each pixel. Simply making the network probabilistic still fails to capture dependencies \emph{among} pixels.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_independent.png}
  \caption[Output of a network that can produce probability distributions]{The output of a network that can produce probability distributions over pixels. Because it has no way to capture dependencies among pixels, the best it can do is to capture the marginal distribution of each pixel. It can specify that the green light can be on or off, and the red light can be on or off, but it cannot specify that \emph{exactly one} of the two must be on.}
  \label{independent}
\end{figure}

The fundamental problem is that to generate images the network would have to output the full joint probability distribution of all the pixels (which is definitely not as simple as a multivariate Gaussian). This is intractable, so we have to find ways to approximate or simplify it. The three main approaches to generation all solve this problem in different ways.

\subsection{Variational autoencoder}

The VAE \citep{vae} is structured as follows (see Figure~\ref{archvae}): An \nquote{encoder} network turns images into low-dimensional seeds (called \nquote{latent vectors}). A \nquote{decoder} network learns to turn the seeds back into images. Both networks are trained simultaneously end-to-end. The latent vector must be low-dimensional to prevent the network from simply recording the image itself into the vector. So far I have described an ordinary autoencoder: a variational autoencoder adds the constraint that the latent vectors must overall have some known distribution.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/arch_vae.png}
  \caption[Architecture of a VAE]{The basic architecture of a VAE. An encoder compresses input images into a latent vector, and a generator tries to recover the original image. Additionally (not shown) the latent vectors are constrained to have some known distribution. For generating images, we discard the encoder, sample a latent vector from the distribution, and pass it into the generator.}
  \label{archvae}
\end{figure}

How does this structure solve the problem of sampling from a joint distribution? Now, instead of choosing random seeds like in Figure~\ref{randompoints}, we let the network associate seeds (latent vectors) to images. This allows the network to assign similar latent vectors to similar images, and thus take advantage of the structure of the image space. With this architecture, we can expect that latent vectors outside the training set will still produce natural images. The constraint that the latent vectors have a known distribution allows us to sample from the natural image manifold, as we wanted to.

VAEs still suffer from producing blurry samples \citep{vaeunderstanding}. The reason can be illustrated with a diagram (Figure~\ref{vaemean}). Suppose a particular latent vector corresponds to two plausible natural images. The network's best (loss-minimising) option is to output the mean of these two images. There is no force ensuring that every individual sample be a natural image, only that loss is minimised overall -- as long as the mapping of latent vectors to natural images is not perfect, blurring results.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_vaemean.png}
  \caption[Output of a VAE]{The output of a VAE that does not have enough space in the latent vector to encode all the images. If multiple possible data points (say, green and red lights) map to the same latent vector, the network can do no better than to \nquote{reconstruct} this latent vector into the average of the data points that map to it.}
  \label{vaemean}
\end{figure}

\subsection{Generative adversarial networks}

GANs \citep{gan} have produced the most spectacular generation results, causing a surge of interest in the idea \citep{imagefromcaption,unsupervisedgan,structurestylegan,texttoimagegan,imagetoimagegan,progressivegrowing}. A GAN (see Figure~\ref{archgan}) consists of two networks: a \nquote{generator} that takes random seeds (latent vectors) as input and produces natural images as output, and a \nquote{discriminator} that takes natural images \emph{or} generator samples as input and tries to tell which is which. Crucially, the generator is never trained on \emph{particular} images. It is free to learn whatever mapping it wants from latent vectors to images, as long as the outputs fool the discriminator. In essence, instead of training the generator on particular pairs of images, we train a differentiable metric -- the discriminator -- that implicitly compares each of the generator's samples to the entire training set \citep{wgan}. The networks are trained end-to-end. To sample images, we simply sample a latent vector from the same distribution we used at training time.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/arch_gan.png}
  \caption[Architecture of a GAN]{The basic architecture of a GAN. A generator produces images given random latent vectors. A discriminator learns to distinguish between real and generated images. The generator learns to fool the discriminator by generating increasingly realistic images. The two networks compete: hence the name \nquote{adversarial}.}
  \label{archgan}
\end{figure}

How does this structure solve the problem of sampling from a joint distribution? Now the generator has to generate images with global consistency, otherwise the discriminator can beat it. We do not suffer from blurriness because we never ask the generator to generate a \emph{particular} image. Thus, the network never has to compromise and take the mean of the images we want. The flip side is the well-known difficulty with GANs: \emph{mode collapse} \citep{gantechniques}. The basic setup described above provides no incentive for the generator to include the whole image manifold in its range of outputs (Figure~\ref{modecollapse}). It is merely judged on whether the particular images it \emph{does} produce look natural. It is not penalised for deciding to \emph{never} produce a certain image.\footnote{Some authors (e.g. \citet{ganmetrics}) distinguish between \emph{mode collapse} -- when multiple distinct latent vectors come to produce the same output -- and \emph{mode dropping} -- when certain regions of the output space are not represented. These two problems are closely related, and I will use \nquote{mode collapse} to refer to both.} This problem has spawned a lot of research into encouraging \nquote{diversity} in GAN samples, e.g. with additional terms in the loss function \citep{gantechniques,energygan,unrolledgan,wgan}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_modecollapse.png}
  \caption[Output of a mode-collapsed GAN]{The output of a mode-collapsed GAN. The generator is judged merely on whether its outputs are realistic. It has no incentive to make sure that it can generate all possible outputs, so it can cheat by focusing on just a subset of the output space (but modelling that region very well).}
  \label{modecollapse}
\end{figure}

\subsection{Autoregressive models}

Autoregressive models originate from trying to solve the problems of VAEs and GANs: blurriness and mode collapse. The headline autoregressive model is PixelCNN \citep{pixelcnn2}, and variations on the same \citep{pixelcnn3,pixelcnn++,superres,wavenet,bytenet,videopixel}. An autoregressive model generates an image one pixel at a time, with each operation taking previously generated pixels as input (see Figure~\ref{archpixelcnn}). This requires choosing and enforcing a strict ordering on pixels, such that no pixel can ever receive information from pixels \nquote{in the future}. At each step, the network outputs a probability distribution over the next pixel. We sample from this distribution, and the actually selected pixel value becomes part of the input for the next step. Generating an image with $N$ distinct values (including multiple channels) thus takes $N$ forward passes, which must be performed serially. However, while generation cannot be parallelised, the training can be parallelised, since at training time all of the true pixel values are already available. Many variations are possible on this basic idea, like using recurrent neural networks to transmit information from distant pixels \citep{pixelcnn1}, but the standard model is a many-layered convolutional network where the convolutions are carefully \emph{masked} \citep{pixelcnn2} or \emph{shifted} \citep{pixelcnn++} to ensure the ordering is never violated.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/arch_pixelcnn.png}
  \caption[Architecture of an autoregressive model]{The basic architecture of an autoregressive model like PixelCNN. A generator takes a partially-generated image as input, and outputs a probability distribution over the possible values of the next pixel. We sample from this distribution, add the pixel to the image, and feed it back into the generator. It then generates another pixel, and so on.}
  \label{archpixelcnn}
\end{figure}

Generating pixels one at a time lets PixelCNN solve the problem of sampling from a joint distribution. If there are two possibilities for a group of pixels (say: green light or red light) the first pixel in that group to be generated will output an appropriate probability distribution. Once we sample it and feed it as input, all the other pixels are generated \emph{conditional} on the already-generated pixel, and so can safely output 100\% probability of red or green, as appropriate (Figure~\ref{lightsideal}). The network thus never needs to output a blurry image that averages between multiple possibilities, and it can capture dependencies among pixels. It also avoids the problem of mode collapse because (unlike the basic GAN) the network is penalised for failing to generate an image in the training set. Finally, because the model outputs a probability distribution at each step, it is possible to exactly calculate the model's \emph{log-likelihood} -- the exact probability that the model will generate any particular image, typically calculated over the test set. For other models like VAEs and GANs, log-likelihood can only be estimated with an expensive sampling procedure \citep{likelihoodestimation}, and as a result other metrics are used to evaluate their performance \citep{ganmetrics}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth]{../../Diagrams/lights_ideal.png}
  \caption[Output of a PixelCNN]{The output of a PixelCNN. It generates the green dimension first, and outputs a $50/50$ chance of the green light being on or off. It then looks at our choice before generating the red dimension. If we choose \emph{on}, it generates a 100\% probability of the red light being off, and vice versa. It thus captures the full joint distribution, including dependencies among pixels.}
  \label{lightsideal}
\end{figure}

\section{Motivation}

The PixelCNN model has three major flaws that I attempt to address. First, generation takes time linear in the number of pixels, since each pixel must be generated to be fed as input to the next pixel. Second, it has an aesthetically displeasing asymmetry. By default, PixelCNN generates pixels from top-left to bottom-right. As a result, PixelCNN can convincingly fill-in the bottom half of an image given the top half (the examples usually displayed in papers \citep{pixelcnn1,quantile}) but not vice versa (unless we train a separate model with the order reversed for that specific task). Third, the model cannot \emph{correct} pixels that are only slightly wrong, and cannot infer which pixels in an image are wrong -- it can \nquote{fill-in} missing parts of an image only if the location of the missing region is known.

The first and second flaws (slow generation, asymmetry) are closely related. They both stem from defining an order on the pixels, and requiring each pixel to depend (only) on the preceding pixels. This is a nice idea, since it allows for an \emph{exact} factorisation of the conditional probability of an image\footnote{PixelCNN factorises the joint probability $p(\vect{x})=p(x_0,\dots,x_n)$ of the image $\vect{x}$ as $p(x_0)p(x_1\mid x_0)\dots p(x_n\mid x_0,\dots,x_{n-1})$} \citep{pixelcnn1}. But it comes with the aforementioned costs. Although the bottom-right pixel can depend on the values of all the others, the top-left pixel can only ever be generated independently, even if we are trying to fill it in and the rest of the image is available. It also means, of course, that pixels have to be generated one at a time.

In theory, we could fill in the early pixels by performing Bayesian inference: given the later pixels in the image, what are the most likely early pixels that could have produced them? However, such inference is computationally intractable. In fact, we use PixelCNN precisely to approximate the ideal Bayesian computation of the conditional likelihoods of all the pixels in the image. Unfortunately, the model is not easily invertible.

The need to maintain a strict ordering of the pixels and prevent information from travelling in the wrong dimensions also makes the model complicated. It is difficult to adapt for a version with any kind of pooling operation, and an image with multiple channels (e.g. red, green, blue) requires all the filters in every layer to be arbitrarily segmented according to which channel information they can access \citep{pixelcnn2}. This complexity makes the model harder to implement, makes it easier to introduce bugs, and makes it harder to experiment with variations on the basic architecture.

What is the alternative? What would it mean if pixels did not have a fixed order? In that case, a generated pixel could depend on the values of \nquote{future} pixels that have not been generated yet i.e. the model would no longer be \nquote{causal}. This seems like a contradiction, but what if we fed some other value as a placeholder for missing pixels? In the simplest possible case, suppose we feed noise\footnote{I suggest using noise instead of zeros, because a region of zeros could also represent an actual black area. However, I have not tested whether this approach fails.}. Then, after the first forward pass, each pixel is generated using only noise as input. But we can do another pass, and now each pixel receives the first generated values as input. On the second pass, we hope that pixels will now be generated to conform to the choices made in the first pass. This procedure can be iterated a few (constant) number of times, and we can hope that after a few passes the generated image will converge to something reasonable\footnote{See section~\ref{markovchain} for a discussion of why a constant number of iterations should be sufficient.}. This procedure is the basis for a relatively less-known family of generative models that decompose the generation process into multiple iterations of a Markov chain. I will discuss them further in section~\ref{markovchain}.

The approach of generating all the pixels in parallel and replacing missing ones with noise has another surprising benefit. It also addresses the third issue (the need to explicitly mark missing pixels). The resulting model simply takes one image and outputs another. The input image can be noise, if we want to generate something from scratch. But it does not have to be. We can also feed a noisy, occluded, or otherwise corrupted version of an image, and try to generate a clean one. This would \emph{not} be possible if we explicitly mask out missing pixels and feed the mask as an additional input to the network (the approach taken by \citet{dnade} and \citet{gsnnade}).

\subsection{Causality}

Because my model violates causality by allowing pixels to be influenced by others that have not been generated, I dub it \nquote{noncausal} PixelCNN. The name draws an analogy to classic noncausal autoregressive models \citep{noncausalimage2,noncausaleco2}. The idea does not apply only to images. In an image, the unnaturalness of defining an order on the pixels is clear. However, the same noncausal approach also applies to text, music, video, and speech. At first glance, strict causality makes more sense for these media, which have a time dimension. Surely it makes sense to have future time steps in an audio stream depend on past ones, and not vice versa? But the fact that we have a time dimension misleads us. When composing speech in real life, we do not always proceed strictly linearly. We may know the conclusion of a paragraph before we have written the middle. Similarly, a composer does not have to write a piece of music in linear order, nor must a director film the scenes of a movie thus. The \nquote{past} parts of these media can depend on the \nquote{future} parts, as we can see from the usefulness of bidirectional RNN models in language tasks \citep{brnnuse1,brnnuse2,brnnuse3}. It makes sense to violate causality when generating these media also, and I hypothesise that the results may have more sophisticated structure, by allowing the model to adjust the beginning to suit the end.

Violating causality even makes sense in the classic application of autoregressive models: the stock market \citep{noncausaleco1}. Here the causality assumption seems genuinely warranted -- certainly, the future values of the stock market cannot \emph{actually} influence past ones (unlike speech, where the whole thing may be composed before it is uttered, and so the end of a sentence really can influence the start). However, noncausal models can still be useful. There are at least two reasons for this. First, there are many hidden variables affecting the stock market that are not fed into a model that only takes stock prices as input. These hidden variables affect the future also, and thus taking future values as input can effectively give the model more insight into these variables \citep{noncausaleco1}. Because past stock prices do not capture all available information about the world, future prices can give extra information even though current prices do not literally depend on them. Second, stock prices depend a great deal on traders' \emph{predictions} of future prices. Although these predictions do not literally depend on the actual future prices, allowing a model to see the true future prices makes it easier for the model to approximate the predictions that traders make \citep{noncausaleco1}.

My model can be viewed as a version of PixelCNN that violates causality. My preferred variation is one that discards the complexity of PixelCNN and replaces it with a simple, symmetric, convolutional network. However, it is possible to achieve something similar with the PixelCNN architecture. When generating an image, we can simply run all of PixelCNN in parallel, replacing unavailable pixels with noise. I have tested this approach also, but found that it performs poorly, probably because it cannot incorporate dependencies upon later (lower, rightward) pixels when generating earlier ones. When generating pixels one at a time, enforcing an order on the pixels is necessary, but it becomes a liability when generating them noncausally, since it limits the model's expressiveness.

\subsection{Training} \label{trainingsection}

The most difficult aspect of the noncausal model is training. Finding an optimal training approach remains important future work. The training procedure for PixelCNN no longer makes sense. PixelCNN is trained using example images. It is trained to output the correct value for each pixel (or, more precisely, to minimise the cross-entropy between the predicted and actual distribution of pixel values). Because each pixel only gets previous pixels as input, this results in a model that captures the conditional probability distribution of each pixel given the previous ones. This will not work for the noncausal variant. Noncausal PixelCNN cannot prevent a pixel from depending on itself. Thus, if we attempt to use the same training procedure, the model can simply learn the identity, and will be useless for generation or denoising.

Even if we could prevent a pixel from depending on itself, this would not be sufficient. Consider an image where two particular pixels are always the same. The model can learn to read the value of either pixel from the other one, without learning the actual distribution of those values conditional on the rest of the image. PixelCNN solves this problem by enforcing an order on the pixels. Whichever of the two pixels comes second will always be copied from the first, but the first has to be predicted without knowledge of itself or of the second.

My preliminary approach to solving this is based on the work of \citet{denoisinggenerative}. They showed that a network trained to denoise images can be turned into a generative model by iterating it multiple times, like a Markov chain. Surprisingly, they found that the optimal procedure was to \emph{add some noise back in} before every iteration of the denoising network\footnote{The reasoning is basically that it is difficult to train a model to generate images after multiple iterations, because we do not know what the ground truth output should be after \emph{just one} iteration, such that later iterations can improve upon it. However, it is easy to train a model to denoise images. \citet{denoisinggenerative} present a method for converting the latter kind of model into the former. Because the model expects noisy images as input, it turns out to be optimal to renoise them after every iteration.}. I decided to test this empirically by generating both with and without the additional \nquote{renoising} step.

To train noncausal PixelCNN, I feed the network noisy versions of images, and train it to output the originals. I also train it to turn the output of its first iteration into the original image, thus hoping to capture both the \nquote{improve the noisy image} and the \nquote{keep improving on the network's own output} behaviours. I do not attempt to propagate gradients back through the sampling step\footnote{Exploring this possibly might be interesting future work.}, and thus we can treat each forward pass as an independent iteration, with some input (either raw data, or the output of a previous pass) and some desired ground-truth output. There is room for training to reproduce an image from itself, since we want the network to act as the identity on uncorrupted natural images. However, I hypothesise that the uncorrupted parts of the images in the denoising task are enough for that.

At test time, noncausal PixelCNN performs multiple forward passes on an image. After each pass, I either do nothing or add some noise back in. Similarly to \cite{gsnnade}, I reduce the amount of noise over time, in a kind of annealing process. The number of passes is a hyperparameter, and I found that 20 seems to be sufficient in the examples I have tried.

\section{Alternative Views}

Noncausal PixelCNN is conceptually a simplification of PixelCNN that removes the ordering over pixels. However, the model can be understood from other perspectives.

\subsection{Markov chain} \label{markovchain}

Repeatedly applying noncausal PixelCNN to an image to produce a probability distribution over output images is a lot like the transition operator of a Markov chain. In fact, there is a lesser-known family of generative models based on this idea. \citet{denoisingthermo} and \citet{infusion} approached this directly, training a neural network to transition in such a way that after a few iterations it produces a sample from the desired distribution. \citet{brnndenoise} did a similar thing with bidirectional recurrent neural networks. In contrast, \citet{denoisinggenerative} rigorously derived a surprising procedure: rather than merely applying the network repeatedly, they found that they needed to train a network to remove noise, but then to actually \emph{add some noise back in} before every iteration of the denoising network. The combined renoising and denoising step constitutes one transition of the Markov chain.

The latter research came very close to the idea I propose in this work. PixelCNN is based on an older autoregressive model called NADE \citep{nade}. This model had only a single hidden layer and therefore did not have to worry about the complexities of masking. Whereas PixelCNN extended the model with masked convolutions, the inventors of NADE extended it by modifying the training procedure \citep{dnade,cnade}. The new procedure is almost noncausal -- instead of enforcing an ordering on pixels, the model takes a mask of \nquote{missing} pixels as input, and is trained to predict their values using only the others. However, generating an image still takes $O(N)$ time because missing pixels have to be generated one at a time to allow the model to capture dependencies among them. This is where Markov chains come in. \citet{gsnnade} showed that one iteration of this model can be seen as a denoising operator, where the complementary renoising operation is to randomly mask out some of the pixels. Thus, the idea from \citet{denoisinggenerative} applies -- instead of generating pixels one at a time, we can also generate images (drawn from the same distribution) by performing several iterations of masking followed by generation. Although they did not use that term, this effectively defines a noncausal autoregressive model. The idea as \citet{gsnnade} presented it did not apply to PixelCNN since PixelCNN was not trained to decorrupt or denoise images. But my noncausal variant does just that, and so the idea applies.

Another question remains: how many steps through the Markov chain are needed to get good samples? \citet{denoisinggenerative} do not answer this theoretically -- they provide conditions under which we can expect to get good samples \emph{at some point}, and they show empirically that a small number of iterations (20 or so) suffices. I suspect the answer has to do with the kind of dependencies present among the pixels in an image. Recall why we cannot settle for just one iteration: after one iteration, all we can capture are the marginal distributions of each pixel, not any of the relationships among them. After two iterations, we can only capture dependencies one \nquote{level} deep -- we can generate pixels whose values depend on arbitrarily many other pixels, where the \emph{other} pixels are completely independent. I will not attempt to formally define this concept, and leave a rigorous analysis for future work. The $O(N)$ iterations of PixelCNN allow us to capture \emph{arbitrary} dependencies, which is another way of saying that the model factorises the joint distribution exactly. But images do not generally have a dependency structure where it is impossible to guess the last pixel without knowing \emph{every single one} of the other pixels in the image. In other words, being able to model arbitrary dependencies is unnecessary, and too strong. The number of iterations it takes to get good samples out of a Markov chain should be closely related to the \nquote{depth} of the dependencies that \emph{actually} exist in natural images. This is an empirical question that depends on the data we want to model. It may differ for different datasets, but I strongly suspect that the number does \emph{not} increase much when we merely increase the resolution of a particular dataset. I do not expect the dependency structure of $64\times 64$ ImageNet images to be much more complex than that of $32 \times 32$ ones. It is merely \emph{bigger}. Consequently, I expect that the number of iterations remains $O(1)$ as the size of the input data increases, though it may vary for different types of data.

\subsection{Denoising autoencoder}

Due to my chosen training method, noncausal PixelCNN acts as a kind of denoising autoencoder \citep{denoisingautoencoder, stackeddenoising} -- it takes noisy images as input and is trained to produce the true images as output. My model does not have a bottleneck, which explains why a pure (not denoising) version would not work and would simply learn the identity. Also unlike an autoencoder, the model's output is a probability distribution from which we can sample. Finally, the model is intended to be applied to an input iteratively, improving the result each time. In that regard, it is similar to the generative denoising autoencoder of \citet{denoisinggenerative}.

\subsection{Adding a latent vector to PixelCNN}

When generating images, PixelCNN has a single source of randomness: the sampling procedure applied to the output to get the final value for each pixel. The other generative models, VAEs and GANs, have a different source of randomness: the latent vector. Noncausal PixelCNN actually has both: generating an image requires first inputting noise, and then performing multiple iterations, with a sampling step each time. The input noise is akin to a latent vector. It seems likely that having two sources of randomness is an undesirable property (e.g. it makes it hard to use \cites{likelihoodestimation} procedure for estimating log-likelihood), and so I train the network to ignore the details of noise. However, it may be possible to find a variation where both sampling and input noise are meaningful in some way.

\subsection{Projection}

Noncausal PixelCNN can be thought of as a projection operator into the natural image manifold. A true projection operator is a transformation that is \emph{idempotent}: that is, applying it more than once makes no further changes. Thus, a projection defines some manifold over which it has no effect, while all inputs outside the manifold are mapped to somewhere in the manifold. Noncausal PixelCNN is not an ideal projection in that sense, because applying it multiple times continues to improve the result. However, it performs a similar operation of taking inputs that are not natural images, and bringing them closer to the nearest natural image. Ideally, noncausal PixelCNN should act as the identity on images that are already valid. Images that are corrupted may have multiple plausible true versions. In this case, noncausal PixelCNN chooses one of the versions at random. But since this operation requires considering dependencies among pixels, it takes the model multiple iterations to choose a point on the manifold.

Thinking about noncausal PixelCNN as a projection also reveals why the model might be suitable for performing multiple different decorruption tasks. Denoising and inpainting are both examples of taking something that is not a natural image, and finding the \nquote{nearest} natural image (though not with the obvious L2 norm as a metric, but rather some more subtle \nquote{perceptual} distance) \citep{imageinpainting}. A network that has learnt this operation should be able to do all of these tasks, and more (e.g. deblurring, upsampling, etc.). If the projection is done in an appropriate space, using perceptual distance, then it might even be able to do some more complex tasks, like elaborating on a rough sketch. Noncausal PixelCNN as defined here is not ready for such application, but I see it as interesting future work.

\subsection{Recurrent CNN}

Since noncausal PixelCNN receives its own outputs as input, it can be thought of as a recurrent neural network \citep{rnn}. Recurrent convolutional neural networks have been used before \citep{rcnnlabel,rcnnobject,rcnnreinforcement,rcnnsaliency,crfrcnn}, but they use deterministic outputs so that errors can be backpropagated through multiple time steps. In particular, \citet{rcnnlabel} and \citet{rcnnsaliency} think of their recurrence as giving the network a chance to correct its own errors, but neither of these consider the use of a discontinuous probabilistic output step to allow the network to incrementally settle on the parameters of a complex joint distribution.

\subsection{Neural field} \label{neuralfields}

Neurons in multiple areas of the human brain are known to be organised in a particular fashion \citep{douglas2004neuronal}. First, the meaning or output space of that brain region is somehow mapped to the physical neuron space \citep{erlhagen1999distribution,douglas2004neuronal}. For example, different locations in the brain region can represent different parts of the visual field \citep{ellias1975pattern}, or different frequencies of sound \citep{romani1982tonotopic}, or different parts of the body \citep{nakamura1998somatosensory}, or different targets for a reaching action \citep{erlhagen1999distribution,cisek2010neural}, or different types of action \citep{cisek2007cortical}. Second, the strong activation of neurons in one part of the region represents a decision \citep{amari1977dynamics,erlhagen1999distribution,cisek2007cortical}. For example, to perceive an ambiguous stimulus in a certain way \citep{kleinschmidt1998human}, to reach in a particular direction \citep{erlhagen1999distribution,cisek2010neural}, or look at a particular point \citep{kopecz1995saccadic}. Third, when a decision needs to be made, the region starts out with diffuse activations all over, and then converges to a single sharp peak after some time \citep{amari1977dynamics,erlhagen1999distribution,brincat2006dynamic}. This process is faster when the decision is easy, and slower when it is hard \citep{decisions1} (because the options are very similar). The convergence process is influenced both by connections within the region (e.g. neurons for very different actions will inhibit each other, expressing the constraint that two different actions cannot be simultaneously chosen \citep{redgrave1999basal}) and by connections into the region from other brain regions that have some information to contribute \citep{redgrave1999basal}.

Work in computational neuroscience typically models the simple case where the task that a region needs to do is to choose the highest of multiple incoming noisy signals \citep{kopecz1995saccadic,decisions1,decisions2,douglas2004neuronal,cisek2007cortical}. This reproduces the basic observation that difficult decisions take longer -- it takes more time to be sure which of several noisy signals is larger if they are close together. More complex models have appeared in recent years \citep{cisek2010neural,sandamirskaya2010embodied,dnfrobots3,dnfrobots,rcnnbio}. In computer vision, a similar idea has been used in the form of Markov random fields \citep{mrf1}, whose solution process can be thought of as iteratively updating a field of connected neurons \citep{crfrcnn}.

I notice a parallel to noncausal PixelCNN -- we start with some noisy input, and slowly converge to a cleaner version. More noise makes the decision take longer, as does an input with multiple plausible ground truths. We can think of the \nquote{image} that is both input and output to the network as a group of neurons that slowly converge to a particular configuration, with the other layers of the network representing a complex web of inhibitory and excitatory connections between them. Alternatively, other layers could represent other brain regions, which suggests a modification to the architecture: adding recurrent connections of some kind within each layer, so that each layer of the network can converge to some representation over time. Further, the layers could be connected in some other way than a purely linear network, such as by adding a web of skip connections \citep{denselyconnected}.


\section{Summary}
Generating images requires some way for the network to express the dependencies among pixels. PixelCNN does this by generating pixels one at a time. Thus, relationships to previous pixels are captured when generating the current one, and relationships to future pixels are captured when generating those future ones. This approach works, but generation is slow and not parallelisable.

I propose speeding it up by generating the pixels all at once, and then repeatedly regenerating them, taking previously generated values as input. This also allows the model to capture dependencies -- after a few iterations, each pixel has \nquote{seen} roughly what the distribution of all the others is like. The biggest challenge (and remaining open problem) is finding a good training procedure. I tentatively suggest teaching the network to denoise images and treating it as the transition operator of a Markov chain, as proposed by \citet{denoisinggenerative}.

Besides being a simplified, noncausal version of PixelCNN, my model can also be seen as a Markov chain generator, a denoising autoencoder, a projection in image space, a recurrent convolutional network, or a neural field. Each of these views suggests possible improvements -- in particular, the analogy to neural fields suggests the possibility of adding recurrences within each layer.


\chapter{Literature Review}
\label{cha:review}

In this chapter I review related work, focusing on the topics covered in the previous chapter. The main things to consider are the variety of generative models, noncausal autoregression, and neural fields.

\section{Generative Models}

\subsection{PixelCNN}

\citet{nade} introduced the modern variety of neural autoregressive generative models. This first version had only one hidden layer, which made it easy to enforce the ordering of data. They encountered difficulties extending their architecture to a deep network \citep{dnade} because it is more difficult to prevent the information flow from violating causality. Their approach to this was to mask out values that had not been generated yet, and to feed the mask as an extra input to the network. \citet{cnade} introduced a convolutional variant of this architecture. Meanwhile, \citet{pixelcnn1} introduced the PixelCNN family of autoregressive generative models, which instead enforced causality using masked convolutions. In this first paper, they presented both a convolutional and a recurrent variant, and found that the recurrent one performed better. PixelRNN connects the pixels with a grid of recurrent network units. It has the major downside that information from distant pixels has to cross many units, and the gradients are therefore weak. An idea that first appeared here was to do generation in multiple steps -- first generate a low-resolution image, which can then guide the high-resolution generator by (sort of) allowing it to peek ahead as well as look back. \Citet{pixelcnn2} argued that the advantage of the recurrent variant came down to its more sophisticated nonlinearities, and so they introduced the standard version of PixelCNN, which is convolutional but has a complex multiplicative gate nonlinearity instead of the more traditional ReLU units. Further work has focused on extending this basic model. \citet{pixelcnn3} Introduced location-dependent conditioning that allows the model to generate images with a predefined structure (e.g. \emph{these} pixels should belong to a \emph{person}).

\citet{pixelcnn++} made several improvements to the model (dubbed PixelCNN+), most notably a change to the output units. Basic PixelCNN uses a 256-way softmax, which allows it to learn arbitrary distributions over output pixels, but prevents it from automatically recognising the natural \nquote{similarity} between pixels of similar colours. \citet{pixelcnn++} replaced the softmax with a logistic mixture that depends on a much smaller number of parameters (5 or so) and implicitly encodes the assumption that similar pixel values can substitute for each other. They also simplified the dependencies among channels of the same pixel. Basic PixelCNN requires that the filters at each layer be segregated according to which channel they are responsible for (so that they can receive information from previous channels, but not later ones). PixelCNN+ instead makes all filters equal, and simply requires that the channels in the final output have a linear dependency on previously generated channels.

\citet{auxiliary} looked into how to speed up PixelCNN generation. They formalised the general idea of first generating a simpler (e.g. scaled down) version of the target, and then using that as input for the final generation step. They proposed generating a series of increasingly larger resolution images, each taking the previous as input. Crucially, they suggested that it should be possible to have most of the model complexity concentrated in the component that depends only on the lower-resolution image (which only needs to be evaluated once per resolution), leaving only a very simple model in the autoregressive part. Thus, although the result is still $O(N)$, it should be significantly faster than the single-shot PixelCNN. \citet{multiscale} took this idea further and actually improved the time to $O(\log N)$ by generating pixels in four interleaved groups (top-left, top-right, bottom-left, bottom-right of each 2 by 2 block), ignoring all dependencies within each group, and generating the first group recursively from a lower-resolution image. For video, they generated individual frames in $O(1)$ by conditioning the first group of pixels only on previous frames (and not on each other).

WaveNet \citep{wavenet} was a very successful application of the PixelCNN architecture to a different modality: sound waveforms. The model can generate speech or music. This paper also introduced the \emph{dilated convolution} (a convolution that takes every $n$'th pixel as input instead of a contiguous block) for increasing the receptive field of output units to allow them to depend on very distant data points. \citet{videopixel} extended the idea to video. They used ordinary PixelCNN within each frame of a video, but added a recurrent component to pool information across frames. This paper also introduced some even \emph{more} complicated nonlinearities which they called \nquote{multiplicative units}. ByteNet \citep{bytenet} applied the idea to the translation of text. They used dilated convolutions to encode input text into a (variable-length) intermediate representation, and then PixelCNN units to decode the representation (one token at a time) into the target language. \citet{quantile} experimented with a different type of output: one that takes a \emph{quantile} between 0 and 1 as input and then \emph{deterministically} generates an output (i.e. the source of randomness is inserted as input to the network rather than being a part of a sampling process at the end). They argued that this method better captures the perceptual details of an image. \citet{superres} applied the architecture to upscaling images, which has the interesting effect that the network does not just get previously generated pixels as input -- it also gets low-resolution versions of the rest of the image, a bit like the multi-stage process used by \citet{pixelcnn1} and \citet{auxiliary}. \citet{pixelsnail} extended PixelCNN's ability to make use of distant data by adding an attention mechanism. \citet{fbtranslate} improved on the translation results of \citet{bytenet}, also by adding an attention mechanism.

\subsection{GAN}

Since their introduction by \citet{gan}, GANs have exploded in popularity due to their uncanny ability to generate more realistic samples than were possible before. The following are some of the milestones in the field. \citet{imagefromcaption} generated images conditional on text captions and introduced an attention mechanism. \citet{unsupervisedgan} used a GAN for unsupervised learning. \citet{vaegan} combined a VAE with an adversarial training mechanism. \citet{structurestylegan} separated the task of image generation into that of generating the structure and style (both with GANs). \citet{texttoimagegan} extended text-to-image synthesis to the generation of images from detailed descriptions. \citet{preferredinputs} used a GAN to generate inputs that maximally activate a particular neuron in a network while making sure the images remain interpretable (i.e. they stay on the natural image manifold). \citet{manifoldmanipulation} used GANs to constrain edits to lie on the natural image manifold, thus making it easy to edit images without destroying their naturalness. \citet{imagetoimagegan} used a GAN to convert images to other images (e.g. recolour, change the style, etc.). \citet{cyclegan} improved on this by forcing the GAN to simultaneously learn the inverse transformation. \citet{stackgan} generated $256\times 256$ images by first generating a lower-resolution version and then feeding that as input to guide the higher-resolution generator. \citet{progressivegrowing} generated $1024\times 1024$ images by training the networks one layer at a time, while progressively adding larger layers and increasing the resolution of the samples. \citet{vaeganinterpolation} used a GAN to improve an autoencoder's ability to interpolate between training data points. \citet{largegan} trained the largest GAN to date, on $128\times 128$ ImageNet.

GANs suffer from well-known issues of unstable training and mode collapse \citep{gantechniques}. Many groups have proposed solutions to these problems. \citet{gantechniques} allowed the discriminator to peek at other examples in a minibatch to detect if they are too similar to the current example. \citet{energygan} modified the loss to encourage orthogonal minibatches. \citet{unrolledgan} improved the loss by training the generator to optimise not just the current discriminator, but the \emph{predicted} discriminator as it would look like after some number of gradient descent steps. \citet{multiagentgan} addressed mode collapse using multiple generators that are encouraged to differ from each other. \citet{progressivegrowing} encouraged diversity by feeding the variance of each minibatch as an extra input to the discriminator, allowing it to detect when the generator produces samples with too little variety. The work by \citet{wgan} was a major step, introducing a new training procedure that is much less likely to suffer from vanishing gradients or mode collapse. The \nquote{Wasserstein GAN} replaces the discriminator (whose goal is to ultimately assign 1 to all true data and 0 to all generated data) with a critic whose goal is to assign a score that varies continuously from the true data points least represented in the generated set to the generated points least similar to the true set. This approach means the critic always has useful gradients for the generator to use, and discourages mode collapse since the generator can always improve by reducing the distance to the most neglected true data. \citet{improvedwgans} improved convergence of Wasserstein GANs by adding the magnitude of the critic's gradient to the loss.

Quantitatively evaluating GANs is harder than autoregressive models because the log-likelihood is not easily calculated. For any given image, the output of PixelCNN is a probability distribution over each pixel given previous pixels. Thus, we can compute the probability that the model would have generated any particular pixel, and we can multiply the probabilities (or add their logs) to find the probability of generating the whole image. Thus we can evaluate the \nquote{log-likelihood} that a PixelCNN model would have generated the images in a test set. But a GAN does not output probabilities, so it is less clear how to evaluate it. \citet{gantournament} proposed comparing GANs with a tournament between their respective generators and discriminators. \citet{likelihoodestimation} derived a sampling-based approximation to the log-likelihood for GANs and VAEs. \citet{ganmetrics} compared multiple metrics currently in use (though not \cites{likelihoodestimation} method), and recommended \nquote{maximum mean discrepancy} \citep{mmd} as the one that is both computationally tractable and best captures the desiderata of a good GAN. Although their paper was focused on GANs, it really applies to any generative model, since all they looked at were metrics that compare two sets of samples -- the test set and the generated data.

\subsection{VAE}

The variational autoencoder was introduced by \citet{vae} and the training procedure improved upon by \citet{vae2}. \citet{drawvae} used a VAE as part of a larger network for generating images. \citet{vaeflows} further improved on the training procedure. \citet{adversarialvae} used an adversarial network instead of a hand-crafted loss to enforce the correct distribution of latent vectors. \citet{vaegan} combined a VAE with an adversarial training mechanism to improve upon the reconstruction quality component of the loss. \citet{vaecompression} used a VAE to lossily compress data and then reconstruct the details with a generative model. \citet{vaeiaf} incorporated some ideas from PixelCNN into the VAE framework. \citet{vaelossy} also combined a VAE with PixelCNN, to encourage the model to learn more useful features for downstream learning. \citet{pixelvae} used PixelCNN for the decoding stage of a VAE to improve its ability to capture fine details. \citet{vaehieararchy} found a way to make the VAE hierarchical, with multiple levels of latent vectors. \citet{vaeunderstanding} developed a generalisation of the VAE training procedure and analysed reasons for blurry samples. \Citet{vqvae} introduced a quantised version of the VAE that has discrete outputs.

\section{Monte Carlo Generation}

A lesser-known alternative to the major approaches discussed above (and a major inspiration behind my proposal) is the family of generative models based on defining the transition operator of a Markov chain and then repeatedly applying the operator starting from some generic noise. After an initial \emph{burn-in} period, these models produce samples from the desired distribution. \citet{denoisinggenerative} used a denoising autoencoder as their operator -- they observed that repeatedly denoising \emph{and then renoising} an image constituted a Markov chain that would eventually produce the appropriate samples (assuming the denoising autoencoder did a good job of reversing the noise). They also proposed using the renoised outputs of the autoencoder as additional training examples, to further encourage the desired behaviour. \citet{gsn} expanded upon this work and generalised it to the case where the \emph{noise} was replaced with an arbitrary operation, potentially mapping to a different space (and the de-noising was a map back to the image). This framework encompasses (for example) the encoding and decoding steps of a variational autoencoder -- encoding maps the image to a different space, and decoding maps the latent vector back to an image. They also explained \emph{why} an iterative process is useful -- the full joint distribution is usually massively multimodal, while the task of denoising a corrupted image has only one or a few modes. Thus, instead of learning to output a complex joint distribution, the denoising model needs to output only a few (the complexity is moved to the choice of \emph{which} few, which is something neural networks are good for). \citet{gsnnade} applied the idea to autoregressive models, specifically the one defined by \citet{dnade} that trained the network to recover masked values. \cites{dnade} procedure for generating images was to sample and unmask pixels one at a time, requiring $O(n)$ forward passes like PixelCNN. \citet{gsnnade} instead observed that the process of masking pixels could be viewed as a type of noising operation, and proposed a Monte Carlo sampling procedure that repeatedly masked out some pixels at random and then regenerated them with the trained model. This procedure is $O(1)$ if the resulting Markov chain burns in quickly, which the authors confirmed empirically. They also proposed an annealing procedure where the amount of noise added on renoising steps was decreased over time, which allowed the model to initially explore the state space and then settle into a mode. Finally, \citet{transitionstochastic} derived an alternative training procedure for their Markov chain generators -- training the generator to reverse a whole sequence of \nquote{destruction} steps performed by a destruction operator.

Several other groups have done work in this space. \citet{markovinference} generated digits by repeatedly turning images into latent vectors and back again (akin to a VAE or the procedure defined by \citet{gsn}). They used a random sampling step and propagated errors through it. \citet{denoisingthermo} defined a generative model as the reversal of a \nquote{diffusion} process, though this requires a very small step size and thousands of iterations. \citet{brnndenoise} used an iterative denoising procedure to fill in gaps in time series data with a bidirectional RNN. \citet{iterativeclosest} proposed training a denoising operator by starting with a minibatch of random noise and then pairing up the network's outputs with the nearest neighbour in a minibatch of training instances. This procedure approximates the Wasserstein distance and is a novel approach to training a denoising network (or actually, any map from a latent space to a desired distribution). Finally, \citet{infusion} trained a Markov chain transition operator by replacing a small number of pixels in every input with the desired target. This allows them to use the outputs of the denoising network as training examples, even if the original output had no relationship to the target. Unlike \citet{denoisinggenerative}, they do not apply a renoising operation after every iteration.

\section{Noncausal Autoregression}

Autoregressive models are those which model a sequence of data points by assuming that each point depends on previous ones, plus some random noise \citep{autoregressive1}. This approach pre-dates deep learning by decades, although the autoregressive models studied before tended to be relatively simple ones (usually linear) that admit closed-form solutions for their parameters. Autoregressive models have been applied to time series analysis (e.g. for economic data, among many others) and signal processing \citep{timeseries}. Noncausal variants of these models are those where data points can also depend on future ones, producing a circular dependency \citep{noncausallikelihood}. They have been used for image processing \citep{noncausalimage1,noncausalimage2}, signal processing \citep{noncausalsignal}, and, more recently, economics \citep{noncausaleco1,noncausaleco2}. But all of these models, even the most recent ones, are still simple (usually linear), though being noncausal makes even the linear models difficult to manage. \citet{noncausaleco2} derived a process for sampling new data points consistent with previous ones, and despite being a linear model the process involves multiple layers of approximations and an iterative computation. To the best of my knowledge, no-one has attempted to extend the concept of noncausal autoregression to a model of such complexity as a deep neural network.

One particular version of a (simple) noncausal autoregressive model popular in computer vision is the Markov random field \citep{mrf1,mrf2} and the conditional random field \citep{crf1,crf2}. These are models of an image as a set of interacting random variables that depend on their neighbours, used for such things as denoising and segmentation \citep{mrfuse1,mrfuse2}. They are usually solved through some form of iterative optimisation procedure \citep{mrflearning1,mrflearning2}. Interestingly, this optimisation procedure can be expressed as a recurrent neural network \citep{crfrcnn}. The dependencies among pixels still tend to be simple linear or quadratic functions. \citet{deepmrf} provided a notable exception. They explored the idea of using a neural network for the dependencies. To find the hidden states, they divided the dependencies into \emph{forward} and \emph{backward} directions, and then iteratively applied both networks, updating the hidden states each time. They thought of this as a pair of coupled recurrent neural networks.

Finally, a model that has the potential to be noncausal autoregressive is the bidirectional RNN \citep{brnn1,brnn2}, often used for language tasks \citep{brnnuse1,brnnuse2,brnnuse3}. This recurrent network takes a sequence as input and then propagates information about the sequence in both directions (backwards and forwards in time). However, the model is not usually used in a way that violates causality \citep{brnndenoise}. Instead, the typical application is to encode a sequence into a (possibly variable-length) latent representation, which is then decoded by something else. In a translation task, a bidirectional RNN might do the encoding stage (understanding the source language) but not the decoding stage (generating the target language), precisely because it is unclear how to use it as a generative model. \citet{brnndenoise} looked into this and came up with solutions similar to what I have proposed. They suggested two alternatives: one is to replace not-yet-generated values with noise, and to repeatedly sample them from the model's outputs\footnote{The authors did not notice this, but their proposed training procedure suffers from a flaw I discuss in section~\ref{trainingsection}: they train their model to output a single missing word given all the others, which means for common word pairs (e.g. \nquote{vice versa}) the network can simply learn to complete the pair, and will not learn to \emph{generate} the pair when filling in a larger gap.}; another is to add an extra input marking which values are missing, and to train the network to understand the difference between missing and available values. \citet{brnnsearch} instead used a modified version of beam search, which is a technique for finding high-likelihood input sequences given a model that evaluates their likelihood (but cannot directly generate them). \cite{brits} used masking for missing values and taught a bidirectional network to fill them in by simultaneously going from the front and from the back, and adding a term to the loss function to encourage both directions to give the same answer. At test time they simply averaged the predictions of the forward and backward networks.

\section{Neural Fields}

The study of neural fields is primarily a branch of neuroscience rather than AI, meaning these computational models are often intended to be biologically plausible models of neural function rather than being the state-of-the-art algorithm to solve a particular problem. Nevertheless, some authors have applied the architectures to robotics and AI tasks. \citet{ellias1975pattern} observed that neurons in the visual pathway had a particular pattern of excitatory and inhibitory connections that allowed them to settle on a peak of activity while suppressing competing peaks. \citet{amari1977dynamics} turned this into a computational model that remains the archetypal neural field today. The model is a differential equation for the behaviour of a \nquote{field} of neurons, where the position of each neuron in the field represents the value of some variable, and the field's output is the location of centre of mass of its activations. The dynamics of the field are such that it tends to a converge to a single peak representing a decision or a percept. One variation of this architecture that I have reviewed above is the Markov random field -- here too we have many units that excite or inhibit each other, and we apply an iterative process to find the field's final output. However, the Markov random field is usually seen as a set of constraints that must be satisfied, and the output is the values at every point; whereas a neural field explicitly evolves through time, and the output is the location of any peaks in the distribution of activations. Besides their use as models of biological neurons \citep{kopecz1995saccadic,ermentrout1998neural,brincat2006dynamic,sandamirskaya2010embodied}, neural fields have also been used for reinforcement learning \citep{dnfreinforcement}, robotics \citep{dnfrobots2,dnfrobots3,dnfrobots}, and neuromorphic computing \citep{dnfneuromorphic}, among others. A single neural field basically performs a \nquote{max} operation, but many such fields can be wired together (each affecting the others) to produce more complex architectures suitable for AI \citep{dnfneuromorphic}. Such an assembly of neural fields resembles an artificial neural network with many recurrent layers.

\section{Recurrent Convolutional Networks}

Multiple groups have experimented with adding recurrent connections to convolutional networks. \citet{rcnnlabel} took the output of a CNN used for labelling the pixels in an image and fed it back in as input, giving the network a chance to correct its own errors. \citet{rcnnsaliency} did a similar thing to a CNN that predicted the \nquote{saliency} of image pixels. They noted that the saliency map used as input in the first time step acted as a kind of prior for the recurrent CNN to improve upon. \citet{rcnnobject} added recurrent connections within the layers of a CNN used for object recognition, and noted the potential connections to the dynamics of biological neurons. \citet{crfrcnn} used convolutional layers as part of an implementation of a Markov random field. They observed that although the Markov random field has circular dependencies and cannot be expressed as a neural network, the iterative approximation algorithm used to solve the random field \emph{can} be expressed as a recurrent neural network. \citet{rcnnreinforcement} used a recurrent CNN for reinforcement learning. \citet{rcnnbio} used a recurrent CNN as a model of biological object recognition that improves upon the purely feed-forward models we know are unrealistic. \citet{videopixel} used recurrent connections to capture the temporal dimension of video while using a convolutional (PixelCNN) architecture for the spatial dimensions. Finally, \citet{rcnnspeech} used a recurrent convolutional network to process and clean up speech.

\chapter{Methods}
\label{cha:methods}

I performed experiments with the MNIST database of handwritten digits \citep{mnist}. Specifically, I used a binarised MNIST \citep{binarisedmnist}, where each pixel is only black or white, with no grays, like the original PixelCNN paper \citep{pixelcnn1}. I trained three models: the ordinary PixelCNN, my noncausal PixelCNN, and a version of PixelCNN that has the normal architecture but is trained with the same procedure as the noncausal version (I will call this \nquote{denoising PixelCNN}). I tried to use the same hyperparameters for all three, though the architectures are slightly different and not directly comparable. Once trained, I evaluated the networks in two ways: by visual inspection of generated images and with the MMD metric \citep{mmd,ganmetrics}.

Source code for all the experiments is available at \url{https://github.com/dmitry-brizhinev/honours-code}

\section{PixelCNN}

I used a basic version of PixelCNN, similar to the one described by \citet{pixelcnn2}. Figure~\ref{pixelcnn} shows the architecture. There was a vertical and a horizontal stack (to avoid a blind spot \citep{pixelcnn2}). The vertical stack used $4\times 5$ convolutions because in combination with the horizontal stack this made the total number of parameters roughly equivalent to the noncausal variant. The horizontal stack used $1\times 5$ convolutions. Both of these convolutions were shifted to preserve the causal structure (see Figure~\ref{convolutions}). Some additional global shifts were necessary, and are depicted in Figure~\ref{pixelcnn}. Each layer included a residual connection and a skip connection to the end. The final layers were a fully-connected one and a softmax output (with two output neurons per pixel, for white and black). This architecture is based on \citet{pixelcnn2}, with some details taken from the more elaborate description by \citet{pixelcnn3}. Those architectures were designed for colour images, whereas I am working with MNIST, which means the architecture described here is simpler because I do not have to account for dependencies among multiple channels.

\begin{figure}
  \centering
    \begin{subfigure}{0.45\columnwidth}
    	\addtocounter{subfigure}{2}
        \centering
        \caption{Last convolutional layer}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_last_layer.png} 
        \label{pixelcnn:last}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Final fully-connected layers}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_end_layer.png} 
        \label{pixelcnn:end}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    \addtocounter{subfigure}{-4}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{First layer}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_first_layer.png} 
        \label{pixelcnn:first}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Intermediate layers (repeated $13\times$)}
        \includegraphics[width=\linewidth]{../../Diagrams/pixelcnn_middle_layer.png} 
        \label{pixelcnn:middle}
    \end{subfigure}
  \caption[Architecture of PixelCNN]{The architecture I used for PixelCNN. Data flows from bottom to top. Numbers next to the lines represent the number of channels per pixel (the number of pixels stays constant). Dark blue boxes represent shifts. Light green boxes represent convolutions with bias terms, and the two light blue boxes (on the residual connections in the first and last layer) represent convolutions without bias terms. Yellow circles represent elementwise nonlinearities. Red circles represent binary elementwise operations. Blue diamonds represent dividing the channels into two streams. Note that this architecture is simpler than ordinary PixelCNN because it does not have to account for dependencies among multiple channels of the same pixel.}
  \label{pixelcnn}
\end{figure}

\begin{figure}
  \centering
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \caption{Normal $5\times 5$ convolution}
        \includegraphics[width=\linewidth]{../../Diagrams/convolution_normal.png} 
        \label{convolutions:normal}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \caption{Shifted $4\times 5$ convolution}
        \includegraphics[width=\linewidth]{../../Diagrams/convolution_45.png} 
        \label{convolutions:45}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \caption{Shifted $1\times 5$ convolution}
        \includegraphics[width=\linewidth]{../../Diagrams/convolution_15.png} 
        \label{convolutions:15}
    \end{subfigure}
  \caption[Shifted convolutions]{An illustration of the shifted convolutions used for PixelCNN. (\subref{convolutions:normal}) In a normal convolution, the middle pixel in the next layer receives the result of convolving the $5\times 5$ region with a filter. (\subref{convolutions:45}) In this shifted convolution, the bottom pixel receives the result. This ensures that no pixel in any layer ever receives any information about pixels below it. (\subref{convolutions:15}) The right pixel receives the result. This ensures that no pixel in any layer ever receives any information about pixels to the right of it.}
  \label{convolutions}
\end{figure}

I used 15 convolutional layers, each the same dimension as the image (convolutions were zero-padded at the edges) with 16 feature filters per layer. I think of the filter size as being $5\times 5$, though as described above this is split between the horizontal and vertical stacks. The final fully connected layers used 64 features per layer. When sampling, I used a temperature\footnote{See section~\ref{temperature} or \citet{pixelcnn2} for an explanation of the sampling temperature} of $1/1.05$. I trained for 200000 iterations, with 32 instances in each minibatch. I used RMSProp with a fixed learning rate of $1\mathrm{e}{-4}$. The hyperparameters are based on those described by \citet{quantile} and on personal communication with Dr van den Oord.

The training procedure was to perform a forward pass with a training example as input, and then to optimise the cross-entropy between the softmax outputs and the ground truth (the same training example).

At test time, I iterated the model to generate pixels one at a time. I generated 10000 images, the same size as the MNIST test set.

\section{Noncausal PixelCNN}

The structure of noncausal PixelCNN is much simpler, being simply a stack of (residual \citep{resnet}) convolutional layers, with no shifting or masking required (Figure~\ref{noncausal}). I used the same gates as PixelCNN, and included residual and skip connections. I attempted to match the hyperparameters of PixelCNN.

\begin{figure}
  \centering
    \begin{subfigure}{0.35\columnwidth}
    	\addtocounter{subfigure}{2}
        \centering
        \caption{Last convolutional layer}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_last_layer.png} 
        \label{noncausal:last}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \caption{Final fully-connected layers}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_end_layer.png} 
        \label{noncausal:end}
    \end{subfigure}
    %\hfill
    \vspace{1cm}
    \addtocounter{subfigure}{-4}
    \begin{subfigure}{0.35\columnwidth}
        \centering
        \caption{First layer}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_first_layer.png} 
        \label{noncausal:first}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\columnwidth}
        \centering
        \caption{Intermediate layers (repeated $13\times$)}
        \includegraphics[width=\linewidth]{../../Diagrams/noncausal_middle_layer.png} 
        \label{noncausal:middle}
    \end{subfigure}
  \caption[Architecture of noncausal PixelCNN]{The architecture I used for my noncausal PixelCNN. Data flows from bottom to top. Numbers next to the lines represent the number of channels per pixel (the number of pixels stays constant). Light green boxes represent convolutions with bias terms, and the two light blue boxes (on the residual connections in the first and last layer) represent convolutions without bias terms. Yellow circles represent elementwise nonlinearities. Red circles represent binary elementwise operations. Blue diamonds represent dividing the channels into two streams.}
  \label{noncausal}
\end{figure}

I used 15 convolutional layers, each the same dimension as the image (convolutions were zero-padded at the edges) with 16 feature filters per layer and a filter size of $5\times 5$. The final fully connected layers used 64 features per layer. When sampling, I used a temperature of $1/1.05$. I trained for 200000 iterations, with 32 instances in each minibatch. I used RMSProp with a fixed learning rate of $1\mathrm{e}{-4}$.

The training procedure for noncausal PixelCNN was different. Each training example was randomly corrupted with between 5\% and 95\% salt-and-pepper noise\footnote{This is the only kind of noise that makes sense for binarised MNIST. For other datasets I would suggest using something more sophisticated, like additive Gaussian noise.}. The noise pixels were drawn from roughly the same distribution as the pixels in binarised MNIST: each noise pixel was black with probability 0.87 and white otherwise. The resulting corrupted examples were compiled into minibatches, which were fed to the model as input. The model's output after one iteration is a probability distribution -- sampling from this distribution produces a second minibatch, which was fed as input again. Cross-entropy errors were backpropagated from both the first and second iteration outputs. Thus, the training alternated between minibatches of 32 corrupted images, and 32 of the model's own outputs. Following \citet{dnade} and \citet{infusion} I scaled the error of each example by $1/p$, where $p$ was the proportion of pixels that had been replaced with noise. This ensures that the \nquote{easier} examples where most of the pixels are already correct do not have a smaller gradient due to a smaller error at the end.

At test time, I fed the model pure noise and iterated it for 20 iterations. I tried three variations on the generation process. The \nquote{denoise only} approach was to simply apply the model repeatedly 20 times. The \nquote{renoise + denoise} approach, based on \citet{gsnnade}, was to add noise before each application of the model. I applied 95\% noise before the second iteration, and then 5\% less each time (ending with 5\% noise before the 20th). Finally, the \nquote{renoise + 2 denoise} approach was based on the observation that I trained my model for two iterations, which in \cites{gsnnade} framework would mean that a single denoising step should be \emph{two} iterations of my model. I thus applied two iterations of denoising after every renoising step (for a total of 40 forward passes). I generated 10000 images with each method.

I also generated some examples of denoising and inpainting. For the inpainting tasks, I replaced the upper or lower half of the image with noise, and then kept the uncorrupted half of the image fixed while applying the same generation procedure to the other half. For the denoising task, I added 50\% noise and then started the generation procedure midway (from the stage where we only add 45\% noise).

\section{Denoising PixelCNN}

The architecture for the denoising variant was the same as ordinary PixelCNN above. All the hyperparameters were also the same. The training and generation procedures, however, were the same as for noncausal PixelCNN. I likewise tried all three variants of the generation procedure and generated 10000 images with each.

\section{MMD Metric}

Each model provided me with 10000 generated images. To compare them, I used the MMD metric \citep{mmd,ganmetrics}, which compares the generated images to the MNIST test set. This metric requires a kernel function (or distance). Distance in pixel space is inappropriate, and \citet{ganmetrics} recommended using the activations in a deep layer of an image recognition network. Accordingly, I trained a simple convolutional classifier on MNIST and used the activations in its fully-connected layer. The details of its architecture are unimportant (interested readers can consult my source code), but what \emph{is} important is that I only trained it once and used the same classifier to evaluate all three models.

I used the MMD metric because \citet{ganmetrics} recommended it out of all the metrics commonly used to compare generative models. The computation of log-likelihood used for PixelCNN unfortunately does not apply to the noncausal variant. It is also non-trivial to apply \cites{likelihoodestimation} sampling procedure, since their technique assumes that all the randomness is present in the choice of latent vector, and the decoder model is deterministic. This is not true of noncausal PixelCNN either.


\chapter{Results}
\label{cha:result}

Overall, the noncausal approach generates images \emph{much} faster, and the noncausal PixelCNN architecture also trains faster (presumably because it is simpler). However, my model was not able to match the quality of PixelCNN, as it did not capture the full range of possible digits, and generated some non-digit glyphs. I suspect this is because the hyperparameters and training procedure are not optimal. The results are not state-of-the-art, but they do look promising. With a hyperparameter search and better training procedure, noncausal PixelCNN could still be a viable alternative to the plain version.

The denoising PixelCNN (keeping the PixelCNN architecture) performed very badly. I expect this is because it is unable to capture the dependencies of early pixels on later ones.

The renoise + denoise approach based on \citet{gsnnade} worked the best. Adding a second iteration of denoising made things worse.

Table~\ref{table} compares the models quantitatively. Recall that PixelCNN is my baseline, denoising PixelCNN is a \nquote{minimal} noncausal model that keeps the PixelCNN architecture but uses my training procedure, and noncausal PixelCNN is my simplified architecture. The two noncausal models have three generation algorithms. Denoise applies the model repeatedly to its own output. Renoise + denoise adds some noise before each denoising step. Renoise + 2 denoise applies the model twice after each renoising step (and therefore requires twice as many forward passes in general). I also compare the MMD scores to that of 10000 images of pure noise. Training and generation took place on an Amazon Web Services p2.xlarge instance. Generation was done in batches of 100 images.

\begin{table}
  \centering
{
\sffamily

\begin{tabular}{lcccc}
%\toprule
\\[-2ex]
Model & Parameters &  Training Time & Generation Time & MMD Score\\
\midrule 
PixelCNN & 223346 & 8 hours & 7589\,s & 0.05\\

Denoising PixelCNN & 223346 & 8 hours & &\\
\hspace{0.5cm}Denoise Only & & & 198\,s & 0.27\\
\hspace{0.5cm}Renoise + Denoise & & & 207\,s & 0.25\\
\hspace{0.5cm}Renoise + 2 Denoise & & & 378\,s & 0.27\\


Noncausal PixelCNN & 207026 & 5 hours & &\\
\hspace{0.5cm}Denoise Only & & & 127\,s & 0.30\\
\hspace{0.5cm}Renoise + Denoise & & & 148\,s & 0.19\\
\hspace{0.5cm}Renoise + 2 Denoise & & & 296\,s & 0.27\\

Pure Noise & & & & 0.31\\

\bottomrule
\end{tabular}
}
  \caption[Metrics for all model variants]{Metrics for all model variants.}
  \label{table}
\end{table}

Figure~\ref{trainingcurves} shows training curves for all three models. There is no sign of overfitting, and also no indication that more training was necessary.
  
\begin{figure}
  \centering    
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \caption{Plain PixelCNN}
        \includegraphics[width=\linewidth]{../../Diagrams/training_pixelcnn.pdf} 
        \label{trainingcurves:pixelcnn}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \caption{Noncausal and Denoising variations}
        \includegraphics[width=\linewidth]{../../Diagrams/training_noncausal.pdf} 
        \label{trainingcurves:noncausal}
    \end{subfigure}
  \caption[Training curves]{Training curves for the three models I trained. Noncausal and denoising PixelCNN used the same training procedure, so their losses are comparable. Plain PixelCNN uses a different procedure and is plotted separately. None of the plots show signs of overfitting.}
  \label{trainingcurves}
\end{figure}

Figure~\ref{samples} shows 100 samples each from PixelCNN and my noncausal variant. We can see that PixelCNN has learnt to generate images. I have thus replicated the success of \citet{pixelcnn1}. My variant is getting there, but two flaws are apparent. First, many of the glyphs it generated are not readily identifiable as \nquote{digits}, suggesting that the model learnt low-level features but did not master high-level structure. Second, it seems to have a preference for glyphs that cover the whole square, and avoids generating ones or sevens. I suspect this is a defect with the training procedure, and that the model is erroneously interpreting pure noise as a signal that none of the regions of the image should be fully black. The walkback training procedure suggested by \citet{denoisinggenerative} may help to alleviate this -- they indicated that it helped their model to produce better samples when starting from a point very far from the manifold of images.

Fig


\begin{figure}
  \centering    
    \begin{subfigure}{0.46\columnwidth}
        \centering
        \caption{Plain PixelCNN}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_pixelcnn.png} 
        \label{samples:pixelcnn}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.46\columnwidth}
        \centering
        \caption{Noncausal PixelCNN}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal.png} 
        \label{samples:noncausal}
    \end{subfigure}
  \caption[Generated samples]{Samples generated by PixelCNN and my noncausal variant. The noncausal samples show two apparent defects: some of the generated images are not identifiable as digits at all, and the model seems to have a preference for glyphs that roughly fill the whole square (whereas PixelCNN happily generates ones and sevens that are mostly empty spaces).}
  \label{samples}
\end{figure}

Figure~\ref{denoisesamples} shows the generation process for denoising PixelCNN. It fails to generate anything resembling digits.

\begin{figure}
  \centering    
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Denoise only}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_denoise_purenoise-none.png} 
        \label{denoisesamples:none}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Renoise + Denoise}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_denoise_purenoise.png} 
        \label{denoisesamples:renoise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Renoise + 2 Denoise}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_denoise_purenoise-two.png} 
        \label{denoisesamples:two}
    \end{subfigure}
  \caption[Generation process by denoising PixelCNN]{Three variants of the generation process by denoising PixelCNN. None of them seem to succeed. Each row shows the output after an iteration of the model taking the previous row as input.}
  \label{denoisesamples}
\end{figure}

Finally, Figure~\ref{noncausalsamples} shows the generation process for noncausal PixelCNN. We can see that renoise + denoise works a lot better than denoise only. There is a clear progression from exploring the state space in the early (high-noise) iterations, and then fine tuning the result in later (low-noise) iterations. Just based on these images, I do not see a clear difference between one and two denoising iterations, but the extra computation does not seem to help (and the quantitative comparison in Table~\ref{table} suggests it is worse overall).

\begin{figure}
  \centering    
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Denoise only}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_purenoise-none.png} 
        \label{noncausalsamples:none}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Renoise + Denoise}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_purenoise.png} 
        \label{noncausalsamples:renoise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Renoise + 2 Denoise}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_purenoise-two.png} 
        \label{noncausalsamples:two}
    \end{subfigure}
  \caption[Generation process by noncausal PixelCNN]{Three variants of the generation process by noncausal PixelCNN. The middle one seems to work best. The extra computation of two denoising steps does not seem to help. Each row shows the output after an iteration of the model taking the previous row as input.}
  \label{noncausalsamples}
\end{figure}
\clearpage
\section{Decorruption}

\begin{figure}
  \centering    
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Denoising}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_denoise.png} 
        \label{decorruption:denoise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Inpainting Top}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_topgap.png} 
        \label{decorruption:topgap}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Inpainting Bottom}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_bottomgap.png} 
        \label{decorruption:bottomgap}
    \end{subfigure}
  \caption[Decorruption by noncausal PixelCNN]{Three decorruption tasks performed by noncausal PixelCNN. The first row is the ground truth, followed by the corrupted input and then iterations of the model. The denoising task is shorter because we skip straight to 50\% noise. Plain PixelCNN can only perform the last task.}
  \label{decorruption}
\end{figure}

Figure~\ref{decorruption} shows noncausal PixelCNN attempting some tasks that plain PixelCNN cannot do. It seems to perform fairly well. Note that even when its final output is different from the ground truth, it is a reasonable completion given the limited information in the corrupted input.

Figure~\ref{diversity} shows the diversity of samples generated from occluded or noisy inputs. Noncausal PixelCNN is usually able to generate reasonable results, and explores multiple plausible completions. Unlike when starting with pure noise, noncausal PixelCNN has no trouble completing images of digits that do not fill the whole square. This suggests that the model has not ignored those parts of the digit manifold -- it merely has trouble reaching them starting from pure noise. \citet{denoisinggenerative} reported a similar defect (their model did not reach some modes when starting far from the image manifold), and their \nquote{walkback} training procedure should alleviate the problem.

\begin{figure}
  \centering    
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Denoising}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_diversity_denoise.png} 
        \label{diversity:denoise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Inpainting Top}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_diversity_topgap.png} 
        \label{diversity:topgap}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\columnwidth}
        \centering
        \caption{Inpainting Bottom}
        \includegraphics[width=\linewidth]{../../Diagrams/samples_noncausal_diversity_bottomgap.png} 
        \label{diversity:bottomgap}
    \end{subfigure}
  \caption[Diversity of decorruption samples]{Multiple samples from decorruption tasks performed by noncausal PixelCNN. The first column is the ground truth, followed by the corrupted input and then ten samples from independent runs of the model. Plain PixelCNN can only perform the last task.}
  \label{diversity}
\end{figure}

\section{Summary}

Noncausal PixelCNN in its current form does not outperform PixelCNN. However, the improvement in generation time is enormous, and thus the idea deserves further study.

\chapter{Conclusion}
\label{cha:conclusion}

I have presented a model based on PixelCNN that improves on its runtime, reducing the $O(N)$ forward passes needed to generate an image to $O(1)$. It is also simpler, more flexible, and can perform tasks PixelCNN cannot do, like denoising and inpainting the top half of an image.

The model can be considered a form of noncausal autoregression. It is inspired by the work of \citet{gsnnade} on Markov chain generation, and also has links to recurrent convolutional neural networks and neural fields. The core idea is to replace missing pixels with noise, and then treat the network as an operator that repeatedly regenerates all the pixels (taking their current values as input), improving the quality of the resulting image each time.

I have compared the model to a PixelCNN baseline with the same hyperparameters. My model ran much faster and overall produced high quality output, but did not match the quality of PixelCNN's generated samples. I still think the idea holds promise and deserves further investigation.

\section{Future Work}

There are many possible improvements to noncausal PixelCNN and future research directions to explore.

\subsection{Existing architectural improvements}

The version of PixelCNN I used as a baseline is a fairly basic one, without some improvements that have been added since. PixelCNN++ \citep{pixelcnn++} added dropout regularisation and replaced softmax outputs with a logistic mixture that reduces the number of independent output neurons and makes it easier for the network to learn the idea that some colours are closer than others. They also added a bottleneck, which makes the architecture more like an autoencoder. The original PixelCNN team have experimented with class-conditional generation \citep{pixelcnn2}, different gates \citep{videopixel} and dilated convolutions \citep{wavenet}. All of these improvements could apply to noncausal PixelCNN also.

\subsection{Hyperparameter search}

I have reused PixelCNN hyperparameters for the noncausal variant, but it is plausible that the optimal hyperparameters for the noncausal variant are different. This means that my experiments were comparing the optimal version of PixelCNN with a suboptimal version of my own model. A hyperparameter search was beyond the computational resources available, but might produce further improvements. Besides the hyperparameters I have explicitly mentioned in Chapter~\ref{cha:methods}, there are other variations that could be applied, like batch normalisation \citep{batchnormalisation}, L2 regularisation, and gradient clipping \citep{gradientclipping}.

\subsection{Other datasets}

Just like PixelCNN has been applied to other datasets \citep{pixelcnn2,wavenet,bytenet,videopixel}, noncausal PixelCNN can also be applied to them. It would be good to see results on Imagenet \citep{imagenet} and CIFAR \citep{cifar}, and on speech, music, text, and video. In particular, I hypothesise that noncausal PixelCNN can improve on the structural properties of generated sequences (like speech) because it can adapt the beginning of a sequence to suit the end, rather than always having to compose the sequence in temporal order.

Experiments with different sizes of Imagenet (like the $32\times 32$ variant used for PixelCNN \citep{pixelcnn2}) would be needed to test how the number of iterations necessary for good generation depends on the input size. I hypothesise that the number can remain constant (see section~\ref{markovchain}), but in this work I only demonstrate that it is markedly less than $N$. It is possible that more iterations are needed to achieve convergence for larger inputs -- the scaling might be $O(\log{N})$ or even $O(N)$ (but with a much smaller constant than ordinary PixelCNN).

\subsection{Improving on the training procedure}

The training procedure I have used for noncausal PixelCNN is based on \citet{gsnnade}, but somewhat modified with my own ideas. It is worth trying the additional improvements suggested by \citet{denoisinggenerative} and \citet{transitionstochastic} -- particularly the \nquote{walkback} procedure where the network is trained to denoise the renoised versions of its own outputs. Alternatively, other teams have proposed different training procedures. \cites{iterativeclosest} \nquote{closest points} approach and \cites{infusion} \nquote{infusion training} both look promising. The former pairs up nearby examples in the training and output minibatches to approximate the Wasserstein distance. The latter replaces some pixels in a noisy input with the ground truth. It would also be interesting to consider a variant of the architecture where errors are somehow backpropagated through the sampling step. Finally, given that the current version of the model shows a preference for samples that fill the whole image, it is worth trying something more sophisticated than uniform salt-and-pepper noise. For example, the probability of a noise pixel's being black or white could instead depend only on the distribution of values for \emph{that particular pixel}, thus making noise pixels in the corners more likely to be black than ones in the middle, and hopefully reducing the model's bias against having large regions of black in the generated samples.

\subsection{Theoretical analysis}

In this work I have not included a detailed theoretical analysis of noncausal PixelCNN. On the contrary, my goal was to show empirically that we do not \emph{need} to maintain an exact factorisation of the probability distribution as PixelCNN provides, and that we can thus use a simpler architecture.

However, \citet{gsn} have provided a formal theoretical treatment of generation using Markov chain transition operators, and I expect that it would be fruitful to express noncausal PixelCNN in their framework. In particular, it would be useful to prove that the model satisfies the conditions they give under which the Markov chain will have a favourable stationary distribution. Such an analysis might also suggest improvements to the architecture that have not occurred in the process of doing largely empirical work. Even without formal mathematics, just thinking about the rough theoretical ideas I have discussed in this work (like treating the network as a projection or as a neural field, and thinking about the reasons why generative models produce blurred samples) has helped inform the architecture I have designed.

It would also be useful to analyse the number of iterations needed to get good samples out of a Markov chain (which \citet{gsn} have not done), and to try to prove that it remains $O(1)$. This might require formalising the concept of dependency \nquote{depth} that I introduced in section~\ref{markovchain}. The number of iterations required corresponds somehow to the dependency structure among pixels, but exactly how remains an open question. Since I expect this number to depend on the data being modelled, it may also be an interesting metric for the \nquote{complexity} of a particular dataset.

\subsection{Experimental architectural improvements}

There are many possible improvements to the noncausal PixelCNN architecture that are worth trying or thinking more about.

\subsubsection{Recurrence}

As suggested by the discussion comparing the architecture to neural fields (section~\ref{neuralfields}), it may be worth considering recurrent connections within each layer. Rather than traditional recurrent connections, maybe these should be sampled from probability distributions, the same way the output of the current model passes through a sampling step before being fed as input. The result would be more similar to actual neural fields, and would operate slightly differently -- rather than an operation repeatedly applied to an image, we would have to see the network more as a set of abstract representations of the image that influence each other and converge over time.

I strongly suspect there are other ways to improve the architecture with ideas from neural fields that I have not thought of yet.

\subsubsection{Rethinking the use of input noise}

In the current model, the input noise acts as a kind of latent vector that affects the output independently of the sampling procedure. Ideally, I feel that there should only be one canonical source of randomness in the generation procedure. It would be good to rethink the architecture and find a way to eliminate one of the two sources of randomness. I do not think the sampling procedure can be eliminated without reintroducing the problem of bluriness, but this should be investigated.

I am likewise not sure how noise can be eliminated. One possibility is to add an extra channel that flags whether a pixel is a known part of the image or not -- but this reintroduces PixelCNN's weakness of needing to know which pixels are valid. Alternatively, perhaps the network can be trained to use zeros as input for missing pixels, instead of noise. My hypothesis is that this will work poorly because the network will interpret zeros as a region of the image known to be black, but again, this should be investigated.

\subsubsection{Reducing temperature} \label{temperature}

The original PixelCNN used a sampling temperature \citep{pixelcnn2} to slightly distort the softmax probabilities at test time. They used a temperature that slightly equalised the probabilities (making low-probability pixels more likely and high-probability pixels less likely) because they found empirically that this improved the quality of the generated images. I used the same temperature throughout my experiments.

Since we expect the output to get closer and closer to a natural image with more iterations, it might make sense to reduce the sampling temperature with each iteration -- that is, early iterations allow more randomness, while later ones force the network to settle on the most probable option.

Another way to accomplish this might be with some form of sparsity regularisation \citep{sparse1,sparse2}. The network as currently structured is a residual network \citep{resnet}. Applying sparsity regularisation is thus similar to limiting the number of pixels that are allowed to change. To do this properly, the final output would need to change -- instead of being expressed as a probability of each possible pixel colour being generated, it would need to express the probability that a pixel \emph{changes} colour. Then we could apply sparsity regularisation to these probabilities, to enforce that most pixels should remain unchanged. It makes sense to add such regularisation, getting stronger with each iteration, to express the fact that we expect more and more pixels to have taken their final value.

\subsubsection{Better losses}

\citet{wgan} introduced the Wasserstein GAN, and showed that while ordinary GANs optimise the KL-divergence between the generated and test images, the WGAN optimises the Wasserstein distance. \citet{quantile} argued that the Wasserstein distance leads to better samples more generally, and proposed a method to apply it to PixelCNN. Their method could easily be applied to noncausal PixelCNN also. Alternatively, \cites{iterativeclosest} training procedure for a denoising operator also approximately optimises the Wasserstein distance.

\subsubsection{Working in the image manifold}

Some more ideas are suggested by the view of noncausal PixelCNN as a projection into the natural image manifold. \citet{manifoldmanipulation} use a GAN to constrain edits to an image to lie on the manifold, by using gradient descent to find a latent vector that corresponds to the image closest to the edit. Similarly, \citet{imageinpainting} use gradient descent to find a latent vector that corresponds to the on-manifold completion of an off-manifold partially occluded image. An improved version of noncausal PixelCNN could do something similar: simply apply a few iterations after an edit to \nquote{return} the result to the manifold, or apply them to an occluded image to get a plausible completion.

Viewing the operation as a quick \nquote{bring this back to the natural image manifold} suggests further ideas. For example, one can start with a rough sketch and then apply multiple iterations of noncausal PixelCNN to obtain a natural image based on the sketch. One could even interleave sketching with projection, making extra edits to move the projection in the desired direction. This procedure might work better if the network is somehow trained to understand the space of images under an appropriate metric -- we want a rough sketch to project to the object being sketched, not some other image with black pixels in the same location (as one would get if finding the nearest neighbour under the L2 norm). I am not sure how to accomplish this, but it is an intriguing possibility for future work.

A general \nquote{natural imagification} operator would be a marvellous achievement. It could simultaneously be used for denoising, completion, deblurring, upsampling, sketching, and editing. And it would represent a new kind of understanding of natural images. Noncausal PixelCNN is a small step in that direction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Here begins the end matter

\backmatter

\bibliographystyle{anuthesis}
\bibliography{thesis}

\printindex

\end{document}
