\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{natbib}
\bibliographystyle{apalike}
%\setcitestyle{authoryear,open={((},close={))}}

\newcommand{\delims}[3]{\!\left #1 #2 \right #3}
\newcommand{\parens}[1]{\delims{(}{#1}{)}}
\newcommand{\braces}[1]{\delims{\lbrace}{#1}{\rbrace}}
\newcommand{\brackets}[1]{\delims{[}{#1}{]}}
\newcommand{\norm}[1]{\delims{\|}{#1}{\|}}
\newcommand{\abs}[1]{\delims{|}{#1}{|}}
\newcommand{\inner}[2]{\delims{\langle}{#1,#2}{\rangle}}
\newcommand{\vectinner}[2]{\inner{\vect{#1}}{\vect{#2}}}

\newcommand{\vect}[1]{\mathbf{#1}}

\begin{document}
\title{\vspace{-10ex}Top-down and Bottom-up Processing}
\author{Dmitry Brizhinev}
\maketitle

\section{Introduction}
I have found that the concept of top-down vs bottom-up processing is widely used in psychology and neuroscience \cite{citationneeded}. Most visual information from human eyes passes via the lateral geniculate nucleus. This is where the first post-retinal processing happens to visual information \cite{citationneeded}. However, most of the \emph{inputs} to the LGN are from higher brain regions \cite{citationneeded}. This suggests that even early processing depends a great deal on top-down cues.

\section{Definitions}
I will explain what I mean by top-down and bottom-up processing using an example. Consider this image \cite{citationneeded}:

\begin{figure}[h!]
  \includegraphics[width=200pt]{thecat.png}
\end{figure}

A human can effortlessly identify this as the text \texttt{THE CAT}. This is despite the fact that the \texttt{H} and \texttt{A} glyphs are identical. Any system that tries to read the text by \emph{first} identifying the letters via combinations of pixels (or strokes), and \emph{then} identifying the words from the letters, is doomed to fail. Such a system can only produce the outputs \texttt{THE CHT} or \texttt{TAE CAT}. [This would be a good place for an empirical demonstration, or for citing one \cite{researchneeded}] To correctly read this text, a system must use the context of the glyph to help identify the glyph. We refer to something like \emph{looking at the pixels in an image to identify the letter} as bottom-up processing, and \emph{looking at the letters around it to figure out what the letter \textbf{should be}} as top-down processing.

A naive model of cognition involves only bottom-up processing. Such models typically define a series of processing steps, each converting a simpler, less meaningful representation into a more complex one, e.g. \emph{pixels $\rightarrow$ strokes $\rightarrow$ letters $\rightarrow$ words $\rightarrow$ sentences}. Top-down processing is any part of the model where information from a ``later" stage in this chain is used to help with an earlier stage. This is possible after the system has made a first guess at the later stage using only bottom-up information, but it might also involve \emph{what the system expected to see}, before any bottom-up information is available.

This entire discussion hinges on the hypothesis that it is possible to clearly delineate ``higher" brain regions, and ``later" stages of processing, \emph{even though} information can flow in any direction. For the early visual system, this is possible -- we can just count the minimum number of synapses it takes for information to travel to a region from the retina. However, perhaps in the frontal cortex these distinctions cease to make sense.

Side note: I am not sure what the relationship is between \emph{top-down/bottom-up processing} as I have defined it, and the \emph{global/local processing} dichotomy also talked about in psychology \cite{citationneeded}.

\section{Approaches to top-down vs bottom-up}
\subsection{Neuroscience and psychology}
The concept of top-down and bottom-up processing originates from psychology. In psychology, top-down is defined something like my definition above. In neuroscience, top-down and bottom-up refer to the direction of information flow (neural connections). It is clear that human cognition involves both kinds of processing, and that top-down processing is very important (rather than a minor part).

\subsection{Coward}


\subsection{Kurzweil}
Kurzweil's architecture proposal \cite{kurzweil} is much less detailed than Coward's, but it pays special attention to top-down processing, using as an example the case of identifying a partially-obscured or corrupted letter in a word \cite[p.~??]{kurzweil}\cite{citationneeded}. Unlike Coward, Kurzweil believes human brain architecture must combine bottom-up and top-down cues, to allow the identification of a letter where neither bit alone is enough. His proposed modules are very similar to Coward's feature extractors, but they do not have a dedicated inhibition circuit to prevent mixing top-down and bottom-up information.

Consider the word \texttt{M??IMUM}. It could be either \texttt{MAXIMUM} or \texttt{MINIMUM}, so top-down processing is not enough to fill in the missing letters (unless the whole word can be inferred from context). Suppose also the missing letters are partially obscured, so bottom-up processing can only produce an ambiguity between \texttt{AX} and \texttt{AY}. Combining both top-down and bottom-up information removes the ambiguities.

\subsection{Minsky}
From what I remember, Minsky's model is a (tree-like) hierarchy of behavioural modules. Thus, I don't think it handles top-down processing very well. However, I will have to go back over the text to make sure \cite{citationneeded}.

\subsection{CNN}
The standard convolutional neural network is strictly bottom-up. In general, any neural net that is based on only one time step of processing will be limited, because it cannot use the results of one time step to make a better decision the second time around (e.g. use the first guess to bias the lower-level neurons in particular ways).

The approach of adding \emph{skip} links joining early layers directly to high level ones \cite{citationneeded} might crudely reproduce top-down processing. If a late layer has access to both high-level features and the raw pixels, it might use the features to re-do the low-level processing. If a network actually learns this function, it will presumably be more efficient to reduce the number of layers and replace it with a different architecture that supports top-down processing explicitly.

Using backpropagation to train a neural network is a form of top-down information flow in a trivial sense -- information flows from \emph{later} layers to \emph{earlier} ones. However, I feel like this doesn't capture the spirit of the idea -- that we can improve our performance at test time (rather than training time) by using the outputs of a later layer to tune an earlier one.

\subsection{RNN}
The recurrent neural network potentially supports top-down processing of some kind, since top-level outputs are fed back in at test time \cite{citationneeded}. If we give the network an input, then wait several steps before expecting an output, the network should in-principle be able to learn the strategy of using processed results to bias lower-level features and produce a better result.

I should look more into some specific RNNs, particularly applications of RNNs to combined optical-character-recognition plus higher processing tasks (e.g. text-to-speech). \cite{researchneeded}

Side note: I find it interesting that neuroscience tends to use the idea of \emph{neural connections going backwards and forwards}, while the AI field has settled on the metaphor of \emph{network outputs feeding into its inputs}, and of \emph{unrolling the RNN into a sequential network with periodic constraints on the weights}.

\subsection{AIXI}
AIXI does not have the concept of \emph{brain regions} or \emph{processing steps}, and so the distinction between top-down and bottom-up processing does not apply to it \cite{citationneeded}. I think of this as an example of the general reason why AIXI is insufficient as a model of \emph{cognition} (not that it's intended to be) -- it defines only the output of the process, not how a realistic system could arrived at the output.

I also want to look at approximate AIXI implementations, to see whether the situation is different there. \cite{researchneeded}



\subsection{Watson}
I don't know enough about the architecture to answer this yet \cite{researchneeded}.

\subsection{AlphaGo}
I don't know enough about the architecture to answer this yet \cite{researchneeded}.

\subsection{Deep Q-learning}
I don't know enough about the architecture to answer this yet \cite{researchneeded}.

%\cite[p.~150--153]{ref} \cite[p.~150]{ref}.





\clearpage

\bibliography{../references}
\end{document}